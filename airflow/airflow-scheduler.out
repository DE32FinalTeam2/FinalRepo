[[34m2024-12-04T17:16:18.854+0900[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-12-04T17:16:18.855+0900[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-12-04T17:16:18.890+0900[0m] {[34mscheduler_job_runner.py:[0m799} INFO[0m - Starting the scheduler[0m
[[34m2024-12-04T17:16:18.892+0900[0m] {[34mscheduler_job_runner.py:[0m806} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-12-04T17:16:18.897+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 468556[0m
[[34m2024-12-04T17:16:18.901+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:16:18.905+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T17:16:18.928+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T17:21:18.954+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:26:18.988+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:31:19.023+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:33:49.013+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 6 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:33:49.013+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:33:49.013+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:33:49.014+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T17:33:49.014+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:33:49.014+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:33:49.014+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:33:49.014+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:33:49.014+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:33:49.015+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:33:49.015+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:33:49.017+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:33:49.017+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:33:49.018+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:33:49.018+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:33:49.018+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:33:49.018+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:33:49.030+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:33:50.169+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:33:50.357+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:34:11.408+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:12.427+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:34:12.610+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:34:13.513+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:14.470+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:34:14.631+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:34:15.412+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-12-04T17:34:15.413+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-12-04T17:34:15.413+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-12-04T17:34:15.418+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:33:50.460907+00:00, run_end_date=2024-12-04 08:34:11.024630+00:00, run_duration=20.563723, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=16, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:33:49.016156+00:00, queued_by_job_id=15, pid=468989[0m
[[34m2024-12-04T17:34:15.419+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:34:12.666290+00:00, run_end_date=2024-12-04 08:34:13.115281+00:00, run_duration=0.448991, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=17, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:33:49.016156+00:00, queued_by_job_id=15, pid=468992[0m
[[34m2024-12-04T17:34:15.419+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:34:14.685933+00:00, run_end_date=2024-12-04 08:34:15.046489+00:00, run_duration=0.360556, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=18, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:33:49.016156+00:00, queued_by_job_id=15, pid=469001[0m
[[34m2024-12-04T17:34:15.603+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:34:15.603+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:34:15.603+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:34:15.603+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T17:34:15.604+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:34:15.604+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:34:15.604+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:34:15.605+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:34:15.605+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:15.606+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:34:15.606+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:15.606+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:34:15.606+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:15.617+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:16.740+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:34:16.921+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:34:44.503+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:34:45.511+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:34:45.719+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:36:01.706+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:36:02.814+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:36:02.974+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:36:03.731+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T17:36:03.731+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-12-04T17:36:03.731+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-12-04T17:36:03.735+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:34:16.977412+00:00, run_end_date=2024-12-04 08:34:44.113984+00:00, run_duration=27.136572, state=failed, executor_state=success, try_number=4, max_tries=3, job_id=19, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:34:15.604818+00:00, queued_by_job_id=15, pid=469005[0m
[[34m2024-12-04T17:36:03.735+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_url_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:34:45.774781+00:00, run_end_date=2024-12-04 08:35:55.303855+00:00, run_duration=69.529074, state=failed, executor_state=success, try_number=3, max_tries=3, job_id=20, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:34:15.604818+00:00, queued_by_job_id=15, pid=469014[0m
[[34m2024-12-04T17:36:03.736+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_joblog_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:36:03.020842+00:00, run_end_date=2024-12-04 08:36:03.341557+00:00, run_duration=0.320715, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=21, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:34:15.604818+00:00, queued_by_job_id=15, pid=469229[0m
[[34m2024-12-04T17:36:03.747+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=468556) last sent a heartbeat 108.18 seconds ago! Restarting it[0m
[[34m2024-12-04T17:36:03.751+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 468556. PIDs of all processes in the group: [468556][0m
[[34m2024-12-04T17:36:03.752+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 468556[0m
[[34m2024-12-04T17:36:03.845+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=468556, status='terminated', exitcode=0, started='17:16:17') (468556) terminated with exit code 0[0m
[[34m2024-12-04T17:36:03.847+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 469231[0m
[[34m2024-12-04T17:36:03.852+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T17:36:03.873+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T17:36:19.052+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:41:19.088+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:46:19.115+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:51:19.146+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T17:51:55.155+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:51:55.155+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:51:55.156+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:51:55.156+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T17:51:55.156+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:51:55.157+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:51:55.157+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:51:55.158+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:51:55.158+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:51:55.158+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:51:55.159+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:51:55.167+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:51:56.153+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:51:56.349+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:51:57.141+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:51:58.270+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:51:58.453+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:51:59.365+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:00.408+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:52:00.581+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [None]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:52:01.004+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T17:52:01.004+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T17:52:01.004+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T17:52:01.007+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:51:56.423454+00:00, run_end_date=2024-12-04 08:51:56.786316+00:00, run_duration=0.362862, state=None, executor_state=success, try_number=4, max_tries=5, job_id=22, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:51:55.156690+00:00, queued_by_job_id=15, pid=469509[0m
[[34m2024-12-04T17:52:01.008+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:51:58.503130+00:00, run_end_date=2024-12-04 08:51:58.915584+00:00, run_duration=0.412454, state=None, executor_state=success, try_number=4, max_tries=5, job_id=23, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:51:55.156690+00:00, queued_by_job_id=15, pid=469512[0m
[[34m2024-12-04T17:52:01.008+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_joblog_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:36:03.020842+00:00, run_end_date=2024-12-04 08:36:03.341557+00:00, run_duration=0.320715, state=None, executor_state=success, try_number=4, max_tries=4, job_id=21, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:51:55.156690+00:00, queued_by_job_id=15, pid=469229[0m
[[34m2024-12-04T17:52:02.139+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 6 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:52:02.139+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:52:02.140+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:52:02.140+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T17:52:02.140+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:52:02.140+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:52:02.140+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:52:02.140+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:52:02.141+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:52:02.141+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:52:02.141+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:52:02.143+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:52:02.144+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:02.144+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:52:02.144+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:02.144+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:52:02.144+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:02.156+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:03.218+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:52:03.391+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:52:24.307+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:25.342+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:52:25.488+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:52:26.284+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:27.343+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:52:27.497+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:52:28.315+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-12-04T17:52:28.316+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-12-04T17:52:28.316+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-12-04T17:52:28.319+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:52:03.444065+00:00, run_end_date=2024-12-04 08:52:23.913714+00:00, run_duration=20.469649, state=up_for_retry, executor_state=success, try_number=5, max_tries=5, job_id=25, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:52:02.141964+00:00, queued_by_job_id=15, pid=469516[0m
[[34m2024-12-04T17:52:28.320+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:52:25.542082+00:00, run_end_date=2024-12-04 08:52:25.904499+00:00, run_duration=0.362417, state=up_for_retry, executor_state=success, try_number=5, max_tries=5, job_id=26, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:52:02.141964+00:00, queued_by_job_id=15, pid=469527[0m
[[34m2024-12-04T17:52:28.320+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:52:27.550709+00:00, run_end_date=2024-12-04 08:52:27.907606+00:00, run_duration=0.356897, state=up_for_retry, executor_state=success, try_number=5, max_tries=5, job_id=27, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:52:02.141964+00:00, queued_by_job_id=15, pid=469530[0m
[[34m2024-12-04T17:52:28.513+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:52:28.514+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:52:28.514+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:52:28.514+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T17:52:28.514+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T17:52:28.514+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T17:52:28.515+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:52:28.516+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:52:28.516+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:28.517+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:52:28.517+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:28.517+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:52:28.517+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:28.528+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:29.568+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:52:29.731+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:52:50.658+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:52:51.603+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:52:51.770+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:10.260+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:11.400+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:55:11.572+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:12.304+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1)[0m
[[34m2024-12-04T17:55:12.305+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T17:55:12.305+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T17:55:12.310+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:52:29.780840+00:00, run_end_date=2024-12-04 08:52:50.254876+00:00, run_duration=20.474036, state=failed, executor_state=success, try_number=6, max_tries=5, job_id=28, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:52:28.515511+00:00, queued_by_job_id=15, pid=469534[0m
[[34m2024-12-04T17:55:12.310+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_url_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:52:51.819006+00:00, run_end_date=2024-12-04 08:55:09.817718+00:00, run_duration=137.998712, state=up_for_retry, executor_state=success, try_number=4, max_tries=4, job_id=29, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:52:28.515511+00:00, queued_by_job_id=15, pid=469545[0m
[[34m2024-12-04T17:55:12.310+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_joblog_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:55:11.627538+00:00, run_end_date=2024-12-04 08:55:11.930624+00:00, run_duration=0.303086, state=up_for_retry, executor_state=success, try_number=4, max_tries=4, job_id=30, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:52:28.515511+00:00, queued_by_job_id=15, pid=469895[0m
[[34m2024-12-04T17:55:12.321+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=469231) last sent a heartbeat 163.85 seconds ago! Restarting it[0m
[[34m2024-12-04T17:55:12.323+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 469231. PIDs of all processes in the group: [469231][0m
[[34m2024-12-04T17:55:12.324+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 469231[0m
[[34m2024-12-04T17:55:12.457+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=469231, status='terminated', exitcode=0, started='17:36:02') (469231) terminated with exit code 0[0m
[[34m2024-12-04T17:55:12.460+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 469897[0m
[[34m2024-12-04T17:55:12.471+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T17:55:12.494+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T17:55:12.668+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:55:12.668+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:55:12.668+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:55:12.668+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T17:55:12.669+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:55:12.670+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:55:12.670+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:12.670+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:55:12.670+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:12.671+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-12-04T17:55:12.671+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:12.683+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:13.751+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:55:13.927+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:14.843+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:15.928+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:55:16.108+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:16.984+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:18.044+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:55:18.222+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:19.118+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1)[0m
[[34m2024-12-04T17:55:19.118+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1)[0m
[[34m2024-12-04T17:55:19.119+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-12-04T17:55:19.124+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:55:13.987807+00:00, run_end_date=2024-12-04 08:55:14.408071+00:00, run_duration=0.420264, state=failed, executor_state=success, try_number=6, max_tries=5, job_id=31, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:55:12.669501+00:00, queued_by_job_id=15, pid=469900[0m
[[34m2024-12-04T17:55:19.124+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:55:16.163643+00:00, run_end_date=2024-12-04 08:55:16.579501+00:00, run_duration=0.415858, state=failed, executor_state=success, try_number=6, max_tries=5, job_id=32, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:55:12.669501+00:00, queued_by_job_id=15, pid=469903[0m
[[34m2024-12-04T17:55:19.125+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=rocketpunch_main_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:55:18.282364+00:00, run_end_date=2024-12-04 08:55:18.655050+00:00, run_duration=0.372686, state=up_for_retry, executor_state=success, try_number=3, max_tries=3, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-04 08:55:12.669501+00:00, queued_by_job_id=15, pid=469906[0m
[[34m2024-12-04T17:55:19.201+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:55:19.201+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T17:55:19.201+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T17:55:19.202+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T17:55:19.203+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:55:19.203+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:19.203+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T17:55:19.204+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:19.213+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:20.344+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:55:20.537+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:37.822+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T17:55:38.839+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T17:55:38.997+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T17:55:39.749+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-12-04T17:55:39.750+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-12-04T17:55:39.753+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_url_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:55:20.604173+00:00, run_end_date=2024-12-04 08:55:37.414150+00:00, run_duration=16.809977, state=failed, executor_state=success, try_number=5, max_tries=4, job_id=34, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:55:19.202508+00:00, queued_by_job_id=15, pid=469909[0m
[[34m2024-12-04T17:55:39.753+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_joblog_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 08:55:39.048593+00:00, run_end_date=2024-12-04 08:55:39.321700+00:00, run_duration=0.273107, state=failed, executor_state=success, try_number=5, max_tries=4, job_id=35, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 08:55:19.202508+00:00, queued_by_job_id=15, pid=470064[0m
[[34m2024-12-04T18:51:28.183+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T18:56:28.230+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T18:57:15.850+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 6 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T18:57:15.850+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T18:57:15.851+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T18:57:15.851+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T18:57:15.851+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T18:57:15.852+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T18:57:15.852+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T18:57:15.852+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T18:57:15.853+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T18:57:15.853+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T18:57:15.853+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T18:57:15.855+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=7, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T18:57:15.855+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:57:15.856+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=7, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T18:57:15.856+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:57:15.856+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=7, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T18:57:15.857+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:57:15.867+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:57:17.122+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T18:57:17.310+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T18:57:38.406+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:57:39.581+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T18:57:39.777+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T18:58:01.159+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:58:03.123+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T18:58:03.454+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T18:58:55.165+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=7, map_index=-1)[0m
[[34m2024-12-04T18:58:55.167+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=7, map_index=-1)[0m
[[34m2024-12-04T18:58:55.168+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=7, map_index=-1)[0m
[[34m2024-12-04T18:58:55.173+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 09:57:17.369876+00:00, run_end_date=2024-12-04 09:57:37.977461+00:00, run_duration=20.607585, state=up_for_retry, executor_state=success, try_number=7, max_tries=7, job_id=36, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 09:57:15.854341+00:00, queued_by_job_id=15, pid=470510[0m
[[34m2024-12-04T18:58:55.173+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 09:57:39.837401+00:00, run_end_date=2024-12-04 09:58:00.695616+00:00, run_duration=20.858215, state=up_for_retry, executor_state=success, try_number=7, max_tries=7, job_id=37, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 09:57:15.854341+00:00, queued_by_job_id=15, pid=470513[0m
[[34m2024-12-04T18:58:55.173+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 09:58:03.551187+00:00, run_end_date=2024-12-04 09:58:54.672007+00:00, run_duration=51.12082, state=up_for_retry, executor_state=success, try_number=7, max_tries=7, job_id=38, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 09:57:15.854341+00:00, queued_by_job_id=15, pid=470546[0m
[[34m2024-12-04T18:58:55.185+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=469897) last sent a heartbeat 99.39 seconds ago! Restarting it[0m
[[34m2024-12-04T18:58:55.188+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 469897. PIDs of all processes in the group: [469897][0m
[[34m2024-12-04T18:58:55.189+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 469897[0m
[[34m2024-12-04T18:58:55.363+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=469897, status='terminated', exitcode=0, started='17:55:11') (469897) terminated with exit code 0[0m
[[34m2024-12-04T18:58:55.370+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 470586[0m
[[34m2024-12-04T18:58:55.377+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T18:58:55.420+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T18:58:55.738+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T18:58:55.738+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T18:58:55.738+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T18:58:55.739+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T18:58:55.739+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T18:58:55.739+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T18:58:55.739+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T18:58:55.739+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T18:58:55.740+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T18:58:55.741+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=8, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T18:58:55.742+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:58:55.743+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=8, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T18:58:55.743+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:58:55.744+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T18:58:55.744+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:58:55.754+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:58:57.204+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T18:58:57.442+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T18:59:18.780+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:59:20.522+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T18:59:20.745+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T18:59:52.487+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T18:59:53.902+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T18:59:54.128+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:02:19.795+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=8, map_index=-1)[0m
[[34m2024-12-04T19:02:19.796+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=8, map_index=-1)[0m
[[34m2024-12-04T19:02:19.796+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1)[0m
[[34m2024-12-04T19:02:19.805+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 09:58:57.509862+00:00, run_end_date=2024-12-04 09:59:18.195598+00:00, run_duration=20.685736, state=failed, executor_state=success, try_number=8, max_tries=7, job_id=39, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 09:58:55.740848+00:00, queued_by_job_id=15, pid=470589[0m
[[34m2024-12-04T19:02:19.805+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 09:59:20.818933+00:00, run_end_date=2024-12-04 09:59:51.959164+00:00, run_duration=31.140231, state=failed, executor_state=success, try_number=8, max_tries=7, job_id=40, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 09:58:55.740848+00:00, queued_by_job_id=15, pid=470598[0m
[[34m2024-12-04T19:02:19.806+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_url_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 09:59:54.207325+00:00, run_end_date=2024-12-04 10:02:19.275784+00:00, run_duration=145.068459, state=success, executor_state=success, try_number=6, max_tries=6, job_id=41, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 09:58:55.740848+00:00, queued_by_job_id=15, pid=470633[0m
[[34m2024-12-04T19:02:19.817+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=470586) last sent a heartbeat 204.14 seconds ago! Restarting it[0m
[[34m2024-12-04T19:02:19.824+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 470586. PIDs of all processes in the group: [470586][0m
[[34m2024-12-04T19:02:19.824+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 470586[0m
[[34m2024-12-04T19:02:19.958+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=470586, status='terminated', exitcode=0, started='18:03:45') (470586) terminated with exit code 0[0m
[[34m2024-12-04T19:02:19.965+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 470962[0m
[[34m2024-12-04T19:02:19.975+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T19:02:20.012+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T19:02:20.034+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:02:20.341+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_data_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:02:20.341+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T19:02:20.341+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T19:02:20.342+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-04T19:02:20.342+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-04T19:02:20.342+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-04T19:02:20.343+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_data_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:02:20.344+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=8, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T19:02:20.345+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:02:20.345+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-04T19:02:20.346+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:02:20.346+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_data_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-12-04T19:02:20.346+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_data_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:02:20.360+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:02:21.982+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:02:22.309+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:02:49.255+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:02:50.680+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:02:50.952+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:03:12.234+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_data_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:03:13.781+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:03:14.116+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_data_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:03:15.656+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=8, map_index=-1)[0m
[[34m2024-12-04T19:03:15.657+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=6, map_index=-1)[0m
[[34m2024-12-04T19:03:15.657+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_data_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-04T19:03:15.664+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:02:22.405882+00:00, run_end_date=2024-12-04 10:02:48.753938+00:00, run_duration=26.348056, state=failed, executor_state=success, try_number=8, max_tries=7, job_id=42, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 10:02:20.343651+00:00, queued_by_job_id=15, pid=470965[0m
[[34m2024-12-04T19:03:15.665+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_data_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:03:14.215392+00:00, run_end_date=2024-12-04 10:03:14.980102+00:00, run_duration=0.76471, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=44, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-04 10:02:20.343651+00:00, queued_by_job_id=15, pid=471027[0m
[[34m2024-12-04T19:03:15.665+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_joblog_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:02:51.040217+00:00, run_end_date=2024-12-04 10:03:11.706036+00:00, run_duration=20.665819, state=success, executor_state=success, try_number=6, max_tries=6, job_id=43, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-04 10:02:20.343651+00:00, queued_by_job_id=15, pid=471000[0m
[[34m2024-12-04T19:03:15.676+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=470962) last sent a heartbeat 55.40 seconds ago! Restarting it[0m
[[34m2024-12-04T19:03:15.680+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 470962. PIDs of all processes in the group: [470962][0m
[[34m2024-12-04T19:03:15.681+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 470962[0m
[[34m2024-12-04T19:03:15.894+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=470962, status='terminated', exitcode=0, started='18:07:10') (470962) terminated with exit code 0[0m
[[34m2024-12-04T19:03:15.900+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 471035[0m
[[34m2024-12-04T19:03:15.910+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T19:03:15.964+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T19:03:16.314+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:03:16.314+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T19:03:16.315+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T19:03:16.316+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:03:16.318+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_crawl_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-12-04T19:03:16.319+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_crawl_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:03:16.320+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-12-04T19:03:16.321+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:03:16.332+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_crawl_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:03:17.793+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:03:18.025+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:03:45.167+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:03:47.161+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:03:47.426+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:04:35.374+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_crawl_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-04T19:04:35.375+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=4, map_index=-1)[0m
[[34m2024-12-04T19:04:35.380+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=rocketpunch_main_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:03:47.496446+00:00, run_end_date=2024-12-04 10:04:34.814934+00:00, run_duration=47.318488, state=up_for_retry, executor_state=success, try_number=4, max_tries=4, job_id=46, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-04 10:03:16.316936+00:00, queued_by_job_id=15, pid=471071[0m
[[34m2024-12-04T19:04:35.380+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_crawl_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:03:18.112380+00:00, run_end_date=2024-12-04 10:03:44.662178+00:00, run_duration=26.549798, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-04 10:03:16.316936+00:00, queued_by_job_id=15, pid=471038[0m
[[34m2024-12-04T19:04:35.391+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=471035) last sent a heartbeat 79.13 seconds ago! Restarting it[0m
[[34m2024-12-04T19:04:35.394+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 471035. PIDs of all processes in the group: [471035][0m
[[34m2024-12-04T19:04:35.395+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 471035[0m
[[34m2024-12-04T19:04:35.528+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=471035, status='terminated', exitcode=0, started='18:08:05') (471035) terminated with exit code 0[0m
[[34m2024-12-04T19:04:35.532+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 471129[0m
[[34m2024-12-04T19:04:35.541+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T19:04:35.576+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T19:04:35.901+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: crawling_to_database.saramin_data_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:04:35.901+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T19:04:35.901+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-04T19:04:35.902+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.saramin_data_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:04:35.906+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_data_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-12-04T19:04:35.907+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_data_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:04:35.907+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_crawl_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-12-04T19:04:35.908+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_crawl_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:04:35.922+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_data_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:04:37.962+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:04:38.225+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_data_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:04:39.692+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_crawl_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:04:41.808+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:04:42.056+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:05:19.335+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_data_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-12-04T19:05:19.336+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_crawl_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-12-04T19:05:19.344+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_data_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:04:38.331465+00:00, run_end_date=2024-12-04 10:04:39.012893+00:00, run_duration=0.681428, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-04 10:04:35.904995+00:00, queued_by_job_id=15, pid=471132[0m
[[34m2024-12-04T19:05:19.345+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_crawl_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:04:42.129456+00:00, run_end_date=2024-12-04 10:05:18.829732+00:00, run_duration=36.700276, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-04 10:04:35.904995+00:00, queued_by_job_id=15, pid=471135[0m
[[34m2024-12-04T19:05:19.673+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:05:19.673+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-04T19:05:19.674+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-04T19:05:19.676+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-12-04T19:05:19.676+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:05:19.690+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-04T19:05:21.329+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-04T19:05:21.641+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-03T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-04T19:06:20.079+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-03T00:00:00+00:00', try_number=5, map_index=-1)[0m
[[34m2024-12-04T19:06:20.085+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=rocketpunch_main_task, run_id=scheduled__2024-12-03T00:00:00+00:00, map_index=-1, run_start_date=2024-12-04 10:05:21.731185+00:00, run_end_date=2024-12-04 10:06:19.462363+00:00, run_duration=57.731178, state=failed, executor_state=success, try_number=5, max_tries=4, job_id=49, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-04 10:05:19.674956+00:00, queued_by_job_id=15, pid=471176[0m
[[34m2024-12-04T19:06:20.096+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=471129) last sent a heartbeat 60.49 seconds ago! Restarting it[0m
[[34m2024-12-04T19:06:20.101+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 471129. PIDs of all processes in the group: [471129][0m
[[34m2024-12-04T19:06:20.101+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 471129[0m
[[34m2024-12-04T19:06:20.315+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=471129, status='terminated', exitcode=0, started='18:09:25') (471129) terminated with exit code 0[0m
[[34m2024-12-04T19:06:20.319+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 471244[0m
[[34m2024-12-04T19:06:20.330+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-04T19:06:20.385+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-04T19:06:20.685+0900[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun crawling_to_database @ 2024-12-03 00:00:00+00:00: scheduled__2024-12-03T00:00:00+00:00, state:running, queued_at: 2024-12-04 08:33:47.483026+00:00. externally triggered: False> failed[0m
[[34m2024-12-04T19:06:20.687+0900[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=crawling_to_database, execution_date=2024-12-03 00:00:00+00:00, run_id=scheduled__2024-12-03T00:00:00+00:00, run_start_date=2024-12-04 08:33:48.095089+00:00, run_end_date=2024-12-04 10:06:20.686746+00:00, run_duration=5552.591657, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-12-03 00:00:00+00:00, data_interval_end=2024-12-04 00:00:00+00:00, dag_hash=48d75e802c7b85ff8dec2f2aa5e69c2d[0m
[[34m2024-12-04T19:06:20.706+0900[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for crawling_to_database to 2024-12-04 00:00:00+00:00, run_after=2024-12-05 00:00:00+00:00[0m
[[34m2024-12-04T19:07:20.087+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:12:20.130+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:17:20.170+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:22:20.213+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:27:20.260+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:32:20.303+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:37:20.346+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:42:20.390+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:47:20.435+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:52:20.482+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T19:57:20.547+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:02:20.595+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:07:20.637+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:12:20.687+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:17:20.731+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:22:20.773+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:27:20.816+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-04T20:32:20.859+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-05T09:12:53.723+0900[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG crawling_to_database is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2024-12-05T09:12:55.427+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 6 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:12:55.429+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-05T09:12:55.430+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-05T09:12:55.431+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-05T09:12:55.431+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-05T09:12:55.432+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-05T09:12:55.441+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-05T09:12:55.441+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-05T09:12:55.442+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-05T09:12:55.443+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-05T09:12:55.444+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:12:55.459+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:12:55.460+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:12:55.461+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:12:55.462+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:12:55.463+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:12:55.463+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:12:55.491+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:13:02.358+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:13:02.902+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:13:42.171+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:13:44.185+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:13:44.527+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:14:07.005+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:14:08.437+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:14:08.782+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:14:32.342+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:14:32.343+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:14:32.343+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:14:32.364+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:13:03.015910+00:00, run_end_date=2024-12-05 00:13:41.721047+00:00, run_duration=38.705137, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:12:55.445872+00:00, queued_by_job_id=15, pid=472796[0m
[[34m2024-12-05T09:14:32.365+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:13:44.627909+00:00, run_end_date=2024-12-05 00:14:06.604690+00:00, run_duration=21.976781, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:12:55.445872+00:00, queued_by_job_id=15, pid=472811[0m
[[34m2024-12-05T09:14:32.365+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:14:08.873325+00:00, run_end_date=2024-12-05 00:14:31.154850+00:00, run_duration=22.281525, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:12:55.445872+00:00, queued_by_job_id=15, pid=472849[0m
[[34m2024-12-05T09:14:32.378+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=471244) last sent a heartbeat 97.17 seconds ago! Restarting it[0m
[[34m2024-12-05T09:14:32.382+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 471244. PIDs of all processes in the group: [471244][0m
[[34m2024-12-05T09:14:32.382+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 471244[0m
[[34m2024-12-05T09:14:32.677+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=471244, status='terminated', exitcode=0, started='18:11:10') (471244) terminated with exit code 0[0m
[[34m2024-12-05T09:14:32.684+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 472883[0m
[[34m2024-12-05T09:14:32.698+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-05T09:14:32.773+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-05T09:14:33.253+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:14:33.253+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-05T09:14:33.254+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-05T09:14:33.254+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-05T09:14:33.254+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-05T09:14:33.255+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-05T09:14:33.255+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 3/3 running and queued tasks[0m
[[34m2024-12-05T09:14:33.255+0900[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - Not executing <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]> since the number of tasks running or queued from DAG crawling_to_database is >= to the DAG's max_active_tasks limit of 3[0m
[[34m2024-12-05T09:14:33.256+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:14:33.258+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:14:33.259+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:14:33.260+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:14:33.260+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:14:33.260+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:14:33.261+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:14:33.270+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'incruit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:14:34.987+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:14:35.185+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.incruit_link_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:15:14.653+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jobkorea_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:15:15.656+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:15:15.829+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jobkorea_link_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:15:37.286+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:15:38.274+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:15:38.459+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:16:24.878+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='incruit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-12-05T09:16:24.878+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jobkorea_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-12-05T09:16:24.879+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:16:24.883+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=incruit_link_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:14:35.256381+00:00, run_end_date=2024-12-05 00:15:14.271812+00:00, run_duration=39.015431, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:14:33.257173+00:00, queued_by_job_id=15, pid=472886[0m
[[34m2024-12-05T09:16:24.884+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jobkorea_link_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:15:15.912875+00:00, run_end_date=2024-12-05 00:15:36.887732+00:00, run_duration=20.974857, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=54, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:14:33.257173+00:00, queued_by_job_id=15, pid=472902[0m
[[34m2024-12-05T09:16:24.884+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_url_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:15:38.505039+00:00, run_end_date=2024-12-05 00:16:24.466579+00:00, run_duration=45.96154, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:14:33.257173+00:00, queued_by_job_id=15, pid=472940[0m
[[34m2024-12-05T09:16:24.895+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=472883) last sent a heartbeat 111.72 seconds ago! Restarting it[0m
[[34m2024-12-05T09:16:24.897+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 472883. PIDs of all processes in the group: [472883][0m
[[34m2024-12-05T09:16:24.898+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 472883[0m
[[34m2024-12-05T09:16:25.031+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=472883, status='terminated', exitcode=0, started='19:41:37') (472883) terminated with exit code 0[0m
[[34m2024-12-05T09:16:25.035+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 473162[0m
[[34m2024-12-05T09:16:25.040+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-05T09:16:25.063+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-05T09:16:25.067+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-05T09:16:25.237+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:16:25.237+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-05T09:16:25.237+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-05T09:16:25.238+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 2/3 running and queued tasks[0m
[[34m2024-12-05T09:16:25.238+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:16:25.239+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:16:25.240+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:16:25.240+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:16:25.240+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:16:25.241+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-12-05T09:16:25.241+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:16:25.247+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'jumpit_link_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:16:26.387+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:16:26.555+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.jumpit_link_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:16:52.850+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_joblog_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:16:53.839+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:16:54.007+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_joblog_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:17:15.001+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'rocketpunch_main_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:17:16.037+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:17:16.210+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.rocketpunch_main_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:19:27.241+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='jumpit_link_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-12-05T09:19:27.242+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_joblog_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:19:27.242+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='rocketpunch_main_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:19:27.246+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=jumpit_link_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:16:26.600328+00:00, run_end_date=2024-12-05 00:16:52.439809+00:00, run_duration=25.839481, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:16:25.238749+00:00, queued_by_job_id=15, pid=473165[0m
[[34m2024-12-05T09:19:27.247+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=rocketpunch_main_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:17:16.262033+00:00, run_end_date=2024-12-05 00:19:26.831590+00:00, run_duration=130.569557, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=58, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-05 00:16:25.238749+00:00, queued_by_job_id=15, pid=473228[0m
[[34m2024-12-05T09:19:27.247+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_joblog_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:16:54.056473+00:00, run_end_date=2024-12-05 00:17:14.610033+00:00, run_duration=20.55356, state=success, executor_state=success, try_number=1, max_tries=1, job_id=57, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:16:25.238749+00:00, queued_by_job_id=15, pid=473192[0m
[[34m2024-12-05T09:19:27.258+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=473162) last sent a heartbeat 182.06 seconds ago! Restarting it[0m
[[34m2024-12-05T09:19:27.260+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 473162. PIDs of all processes in the group: [473162][0m
[[34m2024-12-05T09:19:27.260+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 473162[0m
[[34m2024-12-05T09:19:27.353+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=473162, status='terminated', exitcode=0, started='19:43:29') (473162) terminated with exit code 0[0m
[[34m2024-12-05T09:19:27.357+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 473311[0m
[[34m2024-12-05T09:19:27.362+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-05T09:19:27.391+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-05T09:19:27.565+0900[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:19:27.566+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 0/3 running and queued tasks[0m
[[34m2024-12-05T09:19:27.566+0900[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG crawling_to_database has 1/3 running and queued tasks[0m
[[34m2024-12-05T09:19:27.566+0900[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-12-05T09:19:27.568+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-12-05T09:19:27.568+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:19:27.568+0900[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_crawl_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-12-05T09:19:27.568+0900[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_crawl_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:19:27.578+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'saramin_url_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:19:28.553+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:19:28.717+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.saramin_url_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:20:20.724+0900[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'crawling_to_database', 'wanted_crawl_task', 'scheduled__2024-12-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_crawling_to_db.py'][0m
[[34m2024-12-05T09:20:21.928+0900[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/dohyun/codes/DE32-final/dags/dag_crawling_to_db.py[0m
[[34m2024-12-05T09:20:22.114+0900[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: crawling_to_database.wanted_crawl_task scheduled__2024-12-04T00:00:00+00:00 [queued]> on host LAPTOP-NDMPHCKK.[0m
[[34m2024-12-05T09:20:48.741+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='saramin_url_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-12-05T09:20:48.741+0900[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='crawling_to_database', task_id='wanted_crawl_task', run_id='scheduled__2024-12-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-12-05T09:20:48.745+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=saramin_url_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:19:28.767419+00:00, run_end_date=2024-12-05 00:20:20.109339+00:00, run_duration=51.34192, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=59, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-12-05 00:19:27.567248+00:00, queued_by_job_id=15, pid=473314[0m
[[34m2024-12-05T09:20:48.746+0900[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=crawling_to_database, task_id=wanted_crawl_task, run_id=scheduled__2024-12-04T00:00:00+00:00, map_index=-1, run_start_date=2024-12-05 00:20:22.176276+00:00, run_end_date=2024-12-05 00:20:48.370428+00:00, run_duration=26.194152, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=60, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-05 00:19:27.567248+00:00, queued_by_job_id=15, pid=473515[0m
[[34m2024-12-05T09:20:48.756+0900[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=473311) last sent a heartbeat 81.23 seconds ago! Restarting it[0m
[[34m2024-12-05T09:20:48.758+0900[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 473311. PIDs of all processes in the group: [473311][0m
[[34m2024-12-05T09:20:48.759+0900[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 473311[0m
[[34m2024-12-05T09:20:48.852+0900[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=473311, status='terminated', exitcode=0, started='19:46:32') (473311) terminated with exit code 0[0m
[[34m2024-12-05T09:20:48.855+0900[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 473547[0m
[[34m2024-12-05T09:20:48.860+0900[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-12-05T09:20:48.881+0900] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-12-05T09:21:25.099+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-05T09:26:25.129+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-05T09:31:25.161+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-12-05T09:36:25.209+0900[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
