{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the most recent log file: 20241222.log\n",
      "Reading existing log file: 20241222.log\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=0 (page 0)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=30 (page 1)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=60 (page 2)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=90 (page 3)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=120 (page 4)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=150 (page 5)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=180 (page 6)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=210 (page 7)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m log_process(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# HTTP 요청 보내기\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 직무 공고 링크 추출 (jobdb_info 클래스에 포함된 a 태그)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# incruit 디렉토리 확인 및 생성\n",
    "if not os.path.exists('./incruit'):\n",
    "    os.makedirs('./incruit')  # 디렉토리 생성\n",
    "\n",
    "def log_process(message):\n",
    "    \"\"\"프로세스 상태를 incruit/makelog_process_YYYYMMDD.log 파일에 기록하고 화면에 출력\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    log_file_name = f'./incruit/makelog_process_{today}.log'\n",
    "    print(message)\n",
    "    with open(log_file_name, 'a', encoding='utf-8') as process_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        process_file.write(f\"{timestamp},{message}\\n\")\n",
    "\n",
    "job_url_list = {\n",
    "    \"DE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno={}\"\n",
    "    ],\n",
    "    \"FE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno={}\"\n",
    "    ],\n",
    "    \"BE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno={}\"\n",
    "    ],\n",
    "    \"DA\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno={}\"\n",
    "    ],\n",
    "    \"MLE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno={}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 오늘 날짜로 로그 파일 이름 설정\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "    # 로그 파일을 찾을 디렉토리 설정\n",
    "    log_directory = './incruit'\n",
    "    log_files = [f for f in os.listdir(log_directory) if re.match(r'^\\d{8}\\.log$', f)]\n",
    "\n",
    "    # 가장 최근에 생성된 로그 파일 찾기\n",
    "    if log_files:\n",
    "        log_files.sort(key=lambda x: os.path.getmtime(os.path.join(log_directory, x)), reverse=True)\n",
    "        recent_log_file_name = log_files[0]  # 가장 최근의 로그 파일을 선택\n",
    "        log_process(f\"Found the most recent log file: {recent_log_file_name}\")\n",
    "    else:\n",
    "        log_process(\"No log files found in the directory.\")\n",
    "        recent_log_file_name = None  # recent_log_file_name을 None으로 설정\n",
    "\n",
    "    # 이전 로그 파일이 존재하는지 확인하고 읽기\n",
    "    previous_urls = {}  # 이전 로그에 있는 URL\n",
    "    if recent_log_file_name and os.path.exists(os.path.join(log_directory, recent_log_file_name)):\n",
    "        with open(os.path.join(log_directory, recent_log_file_name), 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            if lines:  # 파일에 내용이 있을 때만 처리\n",
    "                log_process(f\"Reading existing log file: {recent_log_file_name}\")\n",
    "                for line in lines[1:]:  # 첫 번째 줄은 header\n",
    "                    columns = line.strip().split(',')\n",
    "                    if len(columns) >= 5:  # 각 열이 다 있다면\n",
    "                        url = columns[1]\n",
    "                        previous_urls[url] = columns[0]  # 해당 URL의 직무를 저장\n",
    "    else:\n",
    "        log_process(\"No previous log file to read or file is empty.\")\n",
    "\n",
    "    # 오늘 크롤링한 URL을 수집\n",
    "    all_links = []  # 오늘 수집한 모든 링크\n",
    "\n",
    "    # 각 job (키값)에 대한 URL 처리\n",
    "    for job_key, urls in job_url_list.items():\n",
    "        for url in urls:\n",
    "            start_page = 0\n",
    "            end_page = 38\n",
    "            page_step = 30  # 페이지네이션의 startno는 30씩 증가\n",
    "\n",
    "            # 페이지 순차적으로 크롤링\n",
    "            for page in range(start_page, end_page):\n",
    "                startno = page * page_step\n",
    "                formatted_url = url.format(startno)\n",
    "\n",
    "                log_process(f\"Fetching URL: {formatted_url} (page {page})\")\n",
    "                # HTTP 요청 보내기\n",
    "                response = requests.get(formatted_url)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # 직무 공고 링크 추출 (jobdb_info 클래스에 포함된 a 태그)\n",
    "                job_links_on_page = soup.find_all('a', href=True)\n",
    "\n",
    "                for link_tag in job_links_on_page:\n",
    "                    link = link_tag['href']\n",
    "                    if \"jobdb_info\" in link:\n",
    "                        full_link = \"https://www.incruit.com\" + link if link.startswith('/') else link\n",
    "                        if full_link not in all_links:\n",
    "                            all_links.append(full_link)\n",
    "\n",
    "    log_process(\"Crawling completed. Now processing URLs.\")\n",
    "\n",
    "    # 오늘 수집된 URL을 'today_crawled_urls_YYYYMMDD.txt'에 저장\n",
    "    today_crawled_file_name = f'./incruit/today_crawled_urls_{today}.txt'\n",
    "    with open(today_crawled_file_name, 'w', encoding='utf-8') as file:\n",
    "        for url in all_links:\n",
    "            file.write(url + \"\\n\")\n",
    "    log_process(f\"URLs written to {today_crawled_file_name}.\")\n",
    "\n",
    "    # 어제 로그에 존재했던 URL을 'yesterday_existing_urls_YYYYMMDD.txt'에 저장\n",
    "    yesterday_existing_file_name = f'./incruit/yesterday_existing_urls_{today}.txt'\n",
    "    with open(yesterday_existing_file_name, 'w', encoding='utf-8') as file:\n",
    "        for url in previous_urls.keys():\n",
    "            file.write(url + \"\\n\")\n",
    "    log_process(f\"URLs written to {yesterday_existing_file_name}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 오류 발생 시 오류 메시지 기록\n",
    "    error_message = str(e)\n",
    "    log_process(f\"An error occurred: {error_message}\")  \n",
    "    \n",
    "    \n",
    "# 불러온 url 목록보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the most recent log file: 20241221.log\n",
      "Reading existing log file: 20241221.log\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=0 (page 0)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=30 (page 1)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=60 (page 2)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=90 (page 3)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=120 (page 4)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=150 (page 5)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=180 (page 6)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=210 (page 7)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=240 (page 8)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=270 (page 9)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=300 (page 10)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=330 (page 11)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=360 (page 12)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=390 (page 13)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=420 (page 14)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=450 (page 15)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=480 (page 16)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=510 (page 17)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=540 (page 18)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=570 (page 19)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=600 (page 20)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=630 (page 21)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=660 (page 22)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=690 (page 23)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=720 (page 24)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=750 (page 25)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=780 (page 26)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=810 (page 27)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=840 (page 28)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=870 (page 29)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=900 (page 30)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=930 (page 31)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=960 (page 32)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=990 (page 33)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1020 (page 34)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1050 (page 35)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1080 (page 36)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1110 (page 37)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=0 (page 0)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=30 (page 1)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=60 (page 2)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=90 (page 3)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=120 (page 4)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=150 (page 5)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=180 (page 6)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=210 (page 7)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=240 (page 8)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=270 (page 9)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=300 (page 10)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=330 (page 11)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=360 (page 12)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=390 (page 13)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=420 (page 14)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=450 (page 15)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=480 (page 16)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=510 (page 17)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=540 (page 18)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=570 (page 19)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=600 (page 20)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=630 (page 21)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=660 (page 22)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=690 (page 23)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=720 (page 24)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=750 (page 25)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=780 (page 26)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=810 (page 27)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=840 (page 28)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=870 (page 29)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=900 (page 30)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=930 (page 31)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=960 (page 32)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=990 (page 33)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=1020 (page 34)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=1050 (page 35)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=1080 (page 36)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno=1110 (page 37)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=0 (page 0)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=30 (page 1)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=60 (page 2)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=90 (page 3)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=120 (page 4)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=150 (page 5)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=180 (page 6)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=210 (page 7)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=240 (page 8)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=270 (page 9)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=300 (page 10)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=330 (page 11)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=360 (page 12)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=390 (page 13)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=420 (page 14)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=450 (page 15)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=480 (page 16)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=510 (page 17)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=540 (page 18)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=570 (page 19)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=600 (page 20)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=630 (page 21)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=660 (page 22)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=690 (page 23)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=720 (page 24)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=750 (page 25)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=780 (page 26)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=810 (page 27)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=840 (page 28)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=870 (page 29)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=900 (page 30)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=930 (page 31)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=960 (page 32)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=990 (page 33)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=1020 (page 34)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=1050 (page 35)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=1080 (page 36)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno=1110 (page 37)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=0 (page 0)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=30 (page 1)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=60 (page 2)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=90 (page 3)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=120 (page 4)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=150 (page 5)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=180 (page 6)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=210 (page 7)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=240 (page 8)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=270 (page 9)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=300 (page 10)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=330 (page 11)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=360 (page 12)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=390 (page 13)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=420 (page 14)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=450 (page 15)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=480 (page 16)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=510 (page 17)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=540 (page 18)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=570 (page 19)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=600 (page 20)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=630 (page 21)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=660 (page 22)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=690 (page 23)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=720 (page 24)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=750 (page 25)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=780 (page 26)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=810 (page 27)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=840 (page 28)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=870 (page 29)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=900 (page 30)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=930 (page 31)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=960 (page 32)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=990 (page 33)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=1020 (page 34)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=1050 (page 35)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=1080 (page 36)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno=1110 (page 37)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=0 (page 0)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=30 (page 1)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=60 (page 2)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=90 (page 3)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=120 (page 4)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=150 (page 5)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=180 (page 6)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=210 (page 7)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=240 (page 8)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=270 (page 9)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=300 (page 10)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=330 (page 11)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=360 (page 12)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=390 (page 13)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=420 (page 14)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=450 (page 15)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=480 (page 16)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=510 (page 17)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=540 (page 18)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=570 (page 19)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=600 (page 20)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=630 (page 21)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=660 (page 22)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=690 (page 23)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=720 (page 24)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=750 (page 25)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=780 (page 26)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=810 (page 27)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=840 (page 28)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=870 (page 29)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=900 (page 30)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=930 (page 31)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=960 (page 32)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=990 (page 33)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1020 (page 34)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1050 (page 35)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1080 (page 36)\n",
      "Fetching URL: https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno=1110 (page 37)\n",
      "Crawling completed. Now processing URLs.\n",
      "Processed 1487 existing URLs.\n",
      "Processed 14 new URLs.\n",
      "Processed 16 deleted URLs.\n",
      "Log file './incruit/20241222.log' has been written successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# incruit 디렉토리 확인 및 생성\n",
    "if not os.path.exists('./incruit'):\n",
    "    os.makedirs('./incruit')  # 디렉토리 생성\n",
    "\n",
    "def log_error(error_message):\n",
    "    \"\"\"오류를 incruit/makelog_err_YYYYMMDD.log 파일에 기록\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    log_file_name = f'./incruit/makelog_err_{today}.log'\n",
    "    with open(log_file_name, 'a', encoding='utf-8') as err_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        err_file.write(f\"{timestamp},{error_message}\\n\")\n",
    "\n",
    "def log_process(message):\n",
    "    \"\"\"프로세스 상태를 incruit/makelog_process_YYYYMMDD.log 파일에 기록하고 화면에 출력\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    log_file_name = f'./incruit/makelog_process_{today}.log'\n",
    "    print(message)\n",
    "    with open(log_file_name, 'a', encoding='utf-8') as process_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        process_file.write(f\"{timestamp},{message}\\n\")\n",
    "\n",
    "job_url_list = {\n",
    "    \"DE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BF%A3%C1%F6%B4%CF%BE%EE&startno={}\"\n",
    "    ],\n",
    "    \"FE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%C7%C1%B7%D0%C6%AE&startno={}\"\n",
    "    ],\n",
    "    \"BE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B9%E9%BF%A3%B5%E5&startno={}\"\n",
    "    ],\n",
    "    \"DA\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B5%A5%C0%CC%C5%CD+%BA%D0%BC%AE%B0%A1&startno={}\"\n",
    "    ],\n",
    "    \"MLE\": [\n",
    "        \"https://search.incruit.com/list/search.asp?col=job&kw=%B8%D3%BD%C5%B7%AF%B4%D7+%BF%A3%C1%F6%B4%CF%BE%EE&startno={}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 오늘 날짜로 로그 파일 이름 설정\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    today_log_file_name = f\"./incruit/{today}.log\"\n",
    "\n",
    "    # 로그 파일을 찾을 디렉토리 설정\n",
    "    log_directory = './incruit'\n",
    "    log_files = [f for f in os.listdir(log_directory) if re.match(r'^\\d{8}\\.log$', f)]\n",
    "\n",
    "    # 가장 최근에 생성된 로그 파일 찾기\n",
    "    if log_files:\n",
    "        log_files.sort(key=lambda x: os.path.getmtime(os.path.join(log_directory, x)), reverse=True)\n",
    "        recent_log_file_name = log_files[0]  # 가장 최근의 로그 파일을 선택\n",
    "        log_process(f\"Found the most recent log file: {recent_log_file_name}\")\n",
    "    else:\n",
    "        log_process(\"No log files found in the directory. All URLs will be marked as 'update'.\")\n",
    "        recent_log_file_name = None  # recent_log_file_name을 None으로 설정\n",
    "\n",
    "    # 이전 로그 파일이 존재하는지 확인하고 읽기\n",
    "    previous_urls = {}  # 이전 로그에 있는 URL 및 해당 job\n",
    "    if recent_log_file_name and os.path.exists(os.path.join(log_directory, recent_log_file_name)):\n",
    "        with open(os.path.join(log_directory, recent_log_file_name), 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            if lines:\n",
    "                log_process(f\"Reading existing log file: {recent_log_file_name}\")\n",
    "                for line in lines[1:]:  # 첫 번째 줄은 header\n",
    "                    columns = line.strip().split(',')\n",
    "                    if len(columns) >= 5:\n",
    "                        url = columns[1]\n",
    "                        job = columns[0]  # 해당 URL의 job\n",
    "                        notice_status = columns[2]\n",
    "                        if notice_status != \"deleted\":  # \"deleted\" 상태인 URL은 제외\n",
    "                            previous_urls[url] = {\n",
    "                                'job': job,\n",
    "                                'notice_status': notice_status,\n",
    "                                'work_status': columns[3],\n",
    "                                'done_time': columns[4]\n",
    "                            }\n",
    "    else:\n",
    "        log_process(\"No previous log file to read or file is empty.\")\n",
    "\n",
    "    # 오늘 크롤링한 URL을 수집\n",
    "    all_links = []  # 오늘 수집한 모든 링크\n",
    "    job_for_links = {}  # 각 링크에 해당하는 job을 기록하기 위한 dictionary\n",
    "\n",
    "    # 각 job (키값)에 대한 URL 처리\n",
    "    for job_key, urls in job_url_list.items():\n",
    "        for url in urls:\n",
    "            start_page = 0\n",
    "            end_page = 38\n",
    "            page_step = 30  # 페이지네이션의 startno는 30씩 증가\n",
    "\n",
    "            # 페이지 순차적으로 크롤링\n",
    "            for page in range(start_page, end_page):\n",
    "                startno = page * page_step\n",
    "                formatted_url = url.format(startno)\n",
    "\n",
    "                log_process(f\"Fetching URL: {formatted_url} (page {page})\")\n",
    "                # HTTP 요청 보내기\n",
    "                response = requests.get(formatted_url)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # 직무 공고 링크 추출 (jobdb_info 클래스에 포함된 a 태그)\n",
    "                job_links_on_page = soup.find_all('a', href=True)\n",
    "\n",
    "                for link_tag in job_links_on_page:\n",
    "                    link = link_tag['href']\n",
    "                    if \"jobdb_info\" in link:\n",
    "                        full_link = \"https://www.incruit.com\" + link if link.startswith('/') else link\n",
    "                        if full_link not in all_links:\n",
    "                            all_links.append(full_link)\n",
    "                            job_for_links[full_link] = job_key  # 링크에 해당하는 job 기록\n",
    "\n",
    "    log_process(\"Crawling completed. Now processing URLs.\")\n",
    "\n",
    "    # 오늘 크롤링한 URL을 기준으로 상태를 설정\n",
    "    log_data_deleted = []  # deleted 상태를 따로 저장\n",
    "    log_data_other = []    # 나머지 (exist, update) 상태를 따로 저장\n",
    "\n",
    "    # 오늘 크롤링한 링크와 이전 로그의 비교\n",
    "    all_urls_set = set(all_links)\n",
    "    previous_urls_set = set(previous_urls.keys())\n",
    "\n",
    "    # 1. 존재하는 링크 처리 (오늘만 존재)\n",
    "    existing_urls = all_urls_set & previous_urls_set  # 오늘과 어제 모두 존재하는 링크\n",
    "    for url in existing_urls:\n",
    "        notice_status = \"exist\"\n",
    "        work_status = previous_urls[url]['work_status']  # 이전의 상태 그대로\n",
    "        done_time = previous_urls[url]['done_time']  # 이전의 done_time 그대로\n",
    "        job = previous_urls[url]['job']  # 이전 로그에서 job 값 가져오기\n",
    "        log_data_other.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "\n",
    "    log_process(f\"Processed {len(existing_urls)} existing URLs.\")\n",
    "\n",
    "    # 2. 새로 추가된 링크 처리 (오늘만 있는 링크)\n",
    "    new_urls = all_urls_set - previous_urls_set  # 오늘만 존재하는 링크\n",
    "    for url in new_urls:\n",
    "        notice_status = \"update\"\n",
    "        work_status = \"null\"\n",
    "        done_time = \"null\"\n",
    "        job = job_for_links[url]  # 새 URL에는 해당 job_key 값을 사용\n",
    "        log_data_other.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "\n",
    "    log_process(f\"Processed {len(new_urls)} new URLs.\")\n",
    "\n",
    "    # 3. 삭제된 링크 처리 (어제 있었으나 오늘은 없는 링크)\n",
    "    deleted_urls = previous_urls_set - all_urls_set  # 어제는 있었지만 오늘은 없는 것\n",
    "    for url in deleted_urls:\n",
    "        notice_status = \"deleted\"\n",
    "        work_status = \"done\"\n",
    "        done_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')  # 삭제된 시간을 현재 시간으로 설정\n",
    "        job = previous_urls[url]['job']  # 이전 로그에서 job 값 가져오기\n",
    "        log_data_deleted.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "\n",
    "    log_process(f\"Processed {len(deleted_urls)} deleted URLs.\")\n",
    "\n",
    "    # 로그 파일 저장\n",
    "    with open(today_log_file_name, 'w', encoding='utf-8') as file:\n",
    "        # 헤더 작성\n",
    "        file.write(\"job,url,notice_status,work_status,done_time\\n\")\n",
    "        # deleted 항목을 먼저 기록\n",
    "        for line in log_data_deleted:\n",
    "            file.write(line + \"\\n\")\n",
    "        # 나머지 (exist, update) 항목을 그 뒤에 기록\n",
    "        for line in log_data_other:\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "    log_process(f\"Log file '{today_log_file_name}' has been written successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 오류 발생 시 오류 메시지 기록\n",
    "    error_message = str(e)\n",
    "    log_error(error_message)\n",
    "    log_process(f\"An error occurred: {error_message}\")  \n",
    "    \n",
    "    \n",
    " # incruit url 크롤링 완성    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "if not os.path.exists('./wanted'):\n",
    "    os.makedirs('./wanted')  # 디렉토리 생성  \n",
    "\n",
    "def log_error(error_message):\n",
    "    \"\"\"오류를 날짜별로 makelog_err_YYYYMMDD.log 파일에 기록\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')  # 날짜 포맷\n",
    "    log_filename = f'./wanted/makelog_err_{today}.log'  # 파일명에 날짜 추가\n",
    "    with open(log_filename, 'a', encoding='utf-8') as err_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        err_file.write(f\"{timestamp},{error_message}\\n\")\n",
    "\n",
    "def log_process(process_message):\n",
    "    \"\"\"프로세스 로그를 날짜별로 makelog_process_YYYYMMDD.log 파일에 기록\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')  # 날짜 포맷\n",
    "    log_filename = f'./wanted/makelog_process_{today}.log'  # 파일명에 날짜 추가\n",
    "    with open(log_filename, 'a', encoding='utf-8') as process_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        process_file.write(f\"{timestamp},{process_message}\\n\")\n",
    "\n",
    "job_url_list = {\n",
    "    \"DE\": [\n",
    "        \"https://www.wanted.co.kr/search?query=데이터+엔지니어&tab=position\",\n",
    "    ],\n",
    "    \"FE\": [\n",
    "        \"https://www.wanted.co.kr/search?query=프론트엔드&tab=position\"\n",
    "    ],\n",
    "    \"BE\": [\n",
    "        \"https://www.wanted.co.kr/search?query=백엔드&tab=position\"\n",
    "    ],\n",
    "    \"DA\": [\n",
    "        \"https://www.wanted.co.kr/search?query=데이터+분석가&tab=position\",\n",
    "    ],\n",
    "    \"MLE\": [\n",
    "        \"https://www.wanted.co.kr/search?query=머신러닝+엔지니어&tab=position\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 셀레니움 웹 드라이버 설정\n",
    "    options = Options()\n",
    "    options.headless = False  # 드라이버를 헤드리스 모드로 실행할 수 있음 (주석 처리하거나 True로 설정하여 브라우저를 표시하지 않게 할 수 있음)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    log_process(\"Chrome WebDriver started.\")\n",
    "\n",
    "    # 오늘 날짜로 로그 파일 이름 설정\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    today_log_file_name = f\"./wanted/{today}.log\"\n",
    "\n",
    "    # 로그 파일을 찾을 디렉토리 설정\n",
    "    log_directory = './wanted'  # 원하는 디렉토리로 변경\n",
    "    log_files = [f for f in os.listdir(log_directory) if re.match(r'^\\d{8}\\.log$', f)]\n",
    "\n",
    "    # 가장 최근에 생성된 로그 파일 찾기\n",
    "    if log_files:\n",
    "        log_files.sort(key=lambda x: os.path.getmtime(os.path.join(log_directory, x)), reverse=True)\n",
    "        recent_log_file_name = log_files[0]  # 가장 최근의 로그 파일을 선택\n",
    "        log_process(f\"Found the most recent log file: {recent_log_file_name}\")\n",
    "    else:\n",
    "        log_process(\"No log files found in the directory. All URLs will be marked as 'update'.\")\n",
    "        recent_log_file_name = None  # recent_log_file_name을 None으로 설정\n",
    "\n",
    "    # 이전 로그 파일이 존재하는지 확인하고 읽기\n",
    "    previous_urls = {}  # 이전 로그에 있는 URL 및 해당 job\n",
    "    if recent_log_file_name and os.path.exists(os.path.join(log_directory, recent_log_file_name)):\n",
    "        with open(os.path.join(log_directory, recent_log_file_name), 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            if lines:  # 파일에 내용이 있을 때만 처리\n",
    "                for line in lines[1:]:  # 첫 번째 줄은 header\n",
    "                    columns = line.strip().split(',')\n",
    "                    if len(columns) >= 5:  # 만약 각 열이 다 있다면\n",
    "                        url = columns[1]\n",
    "                        job = columns[0]  # 해당 URL의 job\n",
    "                        notice_status = columns[2]\n",
    "                        work_status = columns[3]\n",
    "                        done_time = columns[4]\n",
    "                        if notice_status != \"deleted\":  # \"deleted\" 상태인 URL은 제외\n",
    "                            previous_urls[url] = {'job': job, 'notice_status': notice_status, 'work_status': work_status, 'done_time': done_time}  # URL에 해당하는 정보 저장\n",
    "\n",
    "    # 오늘 크롤링한 URL을 수집\n",
    "    all_links = []  # 오늘 수집한 모든 링크\n",
    "    job_for_links = {}  # 각 링크에 해당하는 job을 기록하기 위한 dictionary\n",
    "\n",
    "    # 각 job (키값)에 대한 URL 처리\n",
    "    for job_key, urls in job_url_list.items():\n",
    "        for url in urls:\n",
    "            log_process(f\"Processing job: {job_key}, URL: {url}\")\n",
    "            # 페이지 열기\n",
    "            driver.get(url)\n",
    "\n",
    "            # 페이지 로딩 대기: 페이지가 완전히 로드될 때까지 기다기\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"a\"))\n",
    "            )\n",
    "            log_process(f\"Page loaded: {url}\")\n",
    "\n",
    "            # 정규 표현식 패턴 (wd/ 뒤에 숫자가 있는 URL을 찾는 패턴)\n",
    "            pattern = re.compile(r'wd/\\d+$')\n",
    "\n",
    "            # 스크롤 내리기 및 링크 추출 반복\n",
    "            previous_height = driver.execute_script(\"return document.body.scrollHeight\")  # 현재 페이지의 높이를 가져옴\n",
    "\n",
    "            while True:\n",
    "                # 페이지에서 모든 <a> 태그를 찾음\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "                # 이미 가져온 링크들을 확인하고 중복되지 않게 추가\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and pattern.search(href) and href not in all_links:\n",
    "                        all_links.append(href)\n",
    "                        job_for_links[href] = job_key  # 링크에 해당하는 job 기록\n",
    "\n",
    "                # 스크롤을 페이지 끝까지 내리기\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                log_process(f\"Scrolling page: {url}\")\n",
    "\n",
    "                # 잠시 대기하여 새로운 요소들이 로드될 시간을 줌\n",
    "                time.sleep(2)  # 2초간 대기, 이 시간은 페이지 로딩 속도에 맞게 조절\n",
    "\n",
    "                # 새로운 페이지 높이가 이전과 같다면 스크롤을 더 이상 내릴 필요가 없으므로 종료\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == previous_height:\n",
    "                    break  # 더 이상 새로운 요소가 로드되지 않으면 반복 종료\n",
    "\n",
    "                previous_height = new_height  # 이전 높이를 업데이트\n",
    "\n",
    "    log_process(f\"Total {len(all_links)} links collected today.\")\n",
    "\n",
    "    # 오늘 크롤링한 URL과 최근 로그 파일을 비교하여 상태 설정\n",
    "    log_data_deleted = []  # deleted 상태를 따로 저장\n",
    "    log_data_other = []    # 나머지 (exist, update) 상태를 따로 저장\n",
    "\n",
    "    # 1. 오늘 크롤링한 URL과 이전 로그 파일의 비교\n",
    "    for url in all_links:\n",
    "        if url in previous_urls:\n",
    "            # 이전 로그 파일과 오늘 모두 존재하는 URL이면 \"exist\"로 처리\n",
    "            if previous_urls[url]['notice_status'] == \"deleted\":\n",
    "                # 이미 'deleted' 상태로 존재하는 공고는 다시 \"deleted\"로 처리하지 않음\n",
    "                continue\n",
    "            notice_status = \"exist\"\n",
    "            work_status = previous_urls[url]['work_status']  # 이전의 상태 그대로\n",
    "            done_time = previous_urls[url]['done_time']  # 이전의 done_time 그대로\n",
    "            job = previous_urls[url]['job']  # 이전 로그에서 job 값 가져오기\n",
    "            log_data_other.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "        else:\n",
    "            # 오늘만 존재하는 URL은 \"update\"로 설정\n",
    "            notice_status = \"update\"\n",
    "            work_status = \"null\"\n",
    "            done_time = \"null\"\n",
    "            job = job_for_links[url]  # 새 URL에는 해당 job_key 값을 사용\n",
    "            log_data_other.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")  # 새 URL에는 해당 job_key 값을 사용\n",
    "\n",
    "    # 2. 이전 로그 파일에 있지만 오늘 로그 파일에 없는 URL 처리\n",
    "    for url in previous_urls:\n",
    "        if url not in all_links:\n",
    "            # 이전에는 존재했지만 오늘은 없는 URL은 \"deleted\"로 설정\n",
    "            if previous_urls[url]['notice_status'] == \"deleted\":\n",
    "                # 이미 'deleted' 상태로 기록된 공고는 다시 'deleted'로 갱신하지 않음\n",
    "                continue\n",
    "            notice_status = \"deleted\"\n",
    "            work_status = \"done\"\n",
    "            done_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')  # 삭제된 시간을 현재 시간으로 설정\n",
    "            job = previous_urls[url]['job']  # 이전 로그에서 job 값 가져오기\n",
    "            log_data_deleted.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")  # 삭제된 URL은 따로 추가\n",
    "\n",
    "    # 로그 파일 저장\n",
    "    with open(today_log_file_name, 'w', encoding='utf-8') as file:\n",
    "        # 헤더 작성\n",
    "        file.write(\"job,url,notice_status,work_status,done_time\\n\")\n",
    "        # deleted 항목을 먼저 기록\n",
    "        for line in log_data_deleted:\n",
    "            file.write(line + \"\\n\")\n",
    "        # 나머지 (exist, update) 항목을 그 뒤에 기록\n",
    "        for line in log_data_other:\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "    log_process(f\"Log file saved: {today_log_file_name}\")\n",
    "\n",
    "    # 브라우저 종료\n",
    "    driver.quit()\n",
    "    log_process(\"Chrome WebDriver closed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 오류 발생 시 오류 메시지 기록\n",
    "    error_message = str(e)\n",
    "    log_error(error_message)\n",
    "    log_process(f\"Error occurred: {error_message}\")\n",
    "    # 프로그램 종료 전 브라우저 종료\n",
    "    if 'driver' in locals():\n",
    "        driver.quit()\n",
    "        log_process(\"Chrome WebDriver closed after error.\")\n",
    "        \n",
    "#wanted 완료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jobkorea/DE_20241222.txt 파일이 존재하지 않아 새로 생성합니다.\n",
      "검색 키워드: 데이터 엔지니어 (DE)\n",
      "81에 수집할 링크가 없으므로 링크 수집을 멈춥니다.\n",
      "파일이 로컬에 성공적으로 저장되었습니다. 경로: ./jobkorea/DE_20241222.txt\n",
      "데이터 엔지니어 (DE)에서 수집한 링크를 로컬에 저장했습니다: ./jobkorea/DE_20241222.txt\n",
      "데이터 엔지니어에서 1165개의 새로운 링크를 발견하였습니다.\n",
      "./jobkorea/FE_20241222.txt 파일이 존재하지 않아 새로 생성합니다.\n",
      "검색 키워드: 프론트엔드 (FE)\n",
      "82에 수집할 링크가 없으므로 링크 수집을 멈춥니다.\n",
      "파일이 로컬에 성공적으로 저장되었습니다. 경로: ./jobkorea/FE_20241222.txt\n",
      "프론트엔드 (FE)에서 수집한 링크를 로컬에 저장했습니다: ./jobkorea/FE_20241222.txt\n",
      "프론트엔드에서 1326개의 새로운 링크를 발견하였습니다.\n",
      "./jobkorea/BE_20241222.txt 파일이 존재하지 않아 새로 생성합니다.\n",
      "검색 키워드: 백엔드 (BE)\n",
      "116에 수집할 링크가 없으므로 링크 수집을 멈춥니다.\n",
      "파일이 로컬에 성공적으로 저장되었습니다. 경로: ./jobkorea/BE_20241222.txt\n",
      "백엔드 (BE)에서 수집한 링크를 로컬에 저장했습니다: ./jobkorea/BE_20241222.txt\n",
      "백엔드에서 1817개의 새로운 링크를 발견하였습니다.\n",
      "./jobkorea/DA_20241222.txt 파일이 존재하지 않아 새로 생성합니다.\n",
      "검색 키워드: 데이터분석가 (DA)\n",
      "7에 수집할 링크가 없으므로 링크 수집을 멈춥니다.\n",
      "파일이 로컬에 성공적으로 저장되었습니다. 경로: ./jobkorea/DA_20241222.txt\n",
      "데이터분석가 (DA)에서 수집한 링크를 로컬에 저장했습니다: ./jobkorea/DA_20241222.txt\n",
      "데이터분석가에서 64개의 새로운 링크를 발견하였습니다.\n",
      "./jobkorea/MLE_20241222.txt 파일이 존재하지 않아 새로 생성합니다.\n",
      "검색 키워드: 머신러닝 엔지니어 (MLE)\n",
      "33에 수집할 링크가 없으므로 링크 수집을 멈춥니다.\n",
      "파일이 로컬에 성공적으로 저장되었습니다. 경로: ./jobkorea/MLE_20241222.txt\n",
      "머신러닝 엔지니어 (MLE)에서 수집한 링크를 로컬에 저장했습니다: ./jobkorea/MLE_20241222.txt\n",
      "머신러닝 엔지니어에서 445개의 새로운 링크를 발견하였습니다.\n",
      "총 4817개의 새로운 링크를 발견하였습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Selenium 웹 드라이버 설정\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# 검색 키워드와 직무 약어 설정\n",
    "search_keywords = {\n",
    "    \"데이터 엔지니어\": \"DE\",\n",
    "    \"프론트엔드\": \"FE\",\n",
    "    \"백엔드\": \"BE\",\n",
    "    \"데이터분석가\": \"DA\",\n",
    "    \"머신러닝 엔지니어\": \"MLE\"\n",
    "}\n",
    "\n",
    "# 오늘 날짜로 txt 파일 이름 설정\n",
    "today_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# ./jobkorea 폴더 생성 (존재하지 않으면 생성)\n",
    "if not os.path.exists('./jobkorea'):\n",
    "    os.makedirs('./jobkorea')\n",
    "\n",
    "# 기존 링크 불러오기 (로컬 파일에서)\n",
    "def load_existing_links(file_path):\n",
    "    existing_links = set()\n",
    "\n",
    "    # 로컬 파일에서 링크 읽기\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            existing_links.update(line.strip() for line in file.readlines())\n",
    "        print(f\"기존 링크를 로드했습니다. 총 {len(existing_links)}개의 링크.\")\n",
    "    else:\n",
    "        print(f\"{file_path} 파일이 존재하지 않아 새로 생성합니다.\")\n",
    "\n",
    "    return existing_links\n",
    "\n",
    "# 로컬에 텍스트 데이터 저장 함수\n",
    "def save_to_local(data, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(data)\n",
    "        print(f\"파일이 로컬에 성공적으로 저장되었습니다. 경로: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {e}\")\n",
    "\n",
    "# 특정 키워드에 대해 링크 크롤링\n",
    "def scrape_links_for_keyword(keyword, keyword_code, existing_links):\n",
    "    base_url = f\"https://www.jobkorea.co.kr/Search/?stext={keyword}&tabType=recruit&Page_No=\"\n",
    "    new_links = []  # 새로 크롤링한 URL 저장\n",
    "    page_number = 1  # 첫 페이지 번호\n",
    "\n",
    "    while True:\n",
    "        # 페이지 열기\n",
    "        url = f\"{base_url}{page_number}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # 페이지 로딩 대기\n",
    "\n",
    "        # 검색 결과가 없으면 종료\n",
    "        try:\n",
    "            no_results_message = driver.find_element(By.CLASS_NAME, \"list-empty-result\")\n",
    "            if \"검색결과가 없습니다\" in no_results_message.text:\n",
    "                print(f\"{page_number}에 수집할 링크가 없으므로 링크 수집을 멈춥니다.\")\n",
    "                break\n",
    "        except Exception:\n",
    "            print(f\"페이지 {page_number}: 검색 결과가 있습니다. 계속 진행합니다...\")\n",
    "\n",
    "        # article class=\"list\" 내부의 링크 가져오기\n",
    "        page_links = []  # 해당 페이지에서 새로 수집한 링크 저장\n",
    "        try:\n",
    "            article_list = driver.find_element(By.CLASS_NAME, \"list\")  # article class=\"list\" 선택\n",
    "            links = article_list.find_elements(By.TAG_NAME, \"a\")  # 해당 article 내부의 <a> 태그 찾기\n",
    "\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and href.startswith(\"https://www.jobkorea.co.kr/Recruit/GI_Read\"):\n",
    "                    # 'PageGbn=HH'가 포함된 링크는 제외\n",
    "                    if \"?PageGbn=HH\" not in href:\n",
    "                        # URL에서 \"?\"로 나눠서 앞부분만 가져오기\n",
    "                        clean_href = href.split('?')[0]\n",
    "                        if clean_href not in existing_links:\n",
    "                            page_links.append(clean_href)  # 직무 코드 없이 URL만 추가\n",
    "                            existing_links.add(clean_href)  # 중복 방지\n",
    "        except Exception as e:\n",
    "            print(f\"페이지 {page_number}에서 링크를 가져오는 데 오류가 발생했습니다: {e}\")\n",
    "\n",
    "        # 해당 페이지의 데이터를 저장\n",
    "        if page_links:\n",
    "            new_links.extend(page_links)\n",
    "\n",
    "        # 페이지 번호 증가\n",
    "        page_number += 1\n",
    "\n",
    "    # 새 링크를 직무 코드별로 로컬 파일에 저장\n",
    "    if new_links:\n",
    "        file_path = f\"./jobkorea/{keyword_code}_{today_date}.txt\"\n",
    "        output_data = \"\\n\".join(new_links)\n",
    "        save_to_local(output_data, file_path)\n",
    "        print(f\"{keyword} ({keyword_code})에서 수집한 링크를 로컬에 저장했습니다: {file_path}\")\n",
    "\n",
    "    return new_links\n",
    "\n",
    "# 모든 키워드에 대해 크롤링\n",
    "def scrape_links():\n",
    "    all_new_links = []\n",
    "\n",
    "    for keyword, keyword_code in search_keywords.items():\n",
    "        # 기존 링크 로드 (각 직무 코드별로 별도 파일)\n",
    "        file_path = f\"./jobkorea/{keyword_code}_{today_date}.txt\"\n",
    "        existing_links = load_existing_links(file_path)\n",
    "        \n",
    "        print(f\"검색 키워드: {keyword} ({keyword_code})\")\n",
    "        new_links = scrape_links_for_keyword(keyword, keyword_code, existing_links)\n",
    "        all_new_links.extend(new_links)\n",
    "        print(f\"{keyword}에서 {len(new_links)}개의 새로운 링크를 발견하였습니다.\")\n",
    "\n",
    "    # 총 결과 출력\n",
    "    print(f\"총 {len(all_new_links)}개의 새로운 링크를 발견하였습니다.\")\n",
    "\n",
    "# 크롤링 실행\n",
    "scrape_links()\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()\n",
    "\n",
    "#잡코리아완료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "팝업 닫음.\n",
      "'데이터 엔지니어' 검색어 입력 완료.\n",
      "링크: https://jumpit.saramin.co.kr/position/40484, D-day: 28\n",
      "링크: https://jumpit.saramin.co.kr/position/40483, D-day: 28\n",
      "링크: https://jumpit.saramin.co.kr/position/40482, D-day: 28\n",
      "링크: https://jumpit.saramin.co.kr/position/40532, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/38294, D-day: 2\n",
      "링크: https://jumpit.saramin.co.kr/position/40534, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/39493, D-day: 13\n",
      "링크: https://jumpit.saramin.co.kr/position/38306, D-day: 2\n",
      "링크: https://jumpit.saramin.co.kr/position/39682, D-day: 22\n",
      "링크: https://jumpit.saramin.co.kr/position/39956, D-day: 21\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/39038, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/38103, D-day: 5\n",
      "링크: https://jumpit.saramin.co.kr/position/40091, D-day: 23\n",
      "링크: https://jumpit.saramin.co.kr/position/40092, D-day: 23\n",
      "링크: https://jumpit.saramin.co.kr/position/40152, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/40153, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/39775, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39777, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39762, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39765, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39756, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39746, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39305, D-day: 13\n",
      "링크: https://jumpit.saramin.co.kr/position/38662, D-day: 6\n",
      "링크: https://jumpit.saramin.co.kr/position/38408, D-day: 4\n",
      "링크: https://jumpit.saramin.co.kr/position/40285, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/39936, D-day: 9\n",
      "링크: https://jumpit.saramin.co.kr/position/39823, D-day: 22\n",
      "링크: https://jumpit.saramin.co.kr/position/39470, D-day: 13\n",
      "링크: https://jumpit.saramin.co.kr/position/39187, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39186, D-day: 12\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/37938, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/40495, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/39728, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/38390, D-day: 3\n",
      "링크: https://jumpit.saramin.co.kr/position/38386, D-day: 4\n",
      "링크: https://jumpit.saramin.co.kr/position/40287, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/39675, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/26709, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/40535, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/38922, D-day: 10\n",
      "링크: https://jumpit.saramin.co.kr/position/23549, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/40377, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/39788, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/39331, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/38369, D-day: 9\n",
      "링크: https://jumpit.saramin.co.kr/position/40572, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/40570, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/38781, D-day: 6\n",
      "링크: https://jumpit.saramin.co.kr/position/39787, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/39726, D-day: 21\n",
      "링크: https://jumpit.saramin.co.kr/position/39323, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/38069, D-day: 5\n",
      "링크: https://jumpit.saramin.co.kr/position/39875, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/39876, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/39877, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/40195, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/39432, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39431, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39428, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39427, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/38458, D-day: 4\n",
      "링크: https://jumpit.saramin.co.kr/position/39963, D-day: 19\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/39060, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/38572, D-day: 5\n",
      "링크: https://jumpit.saramin.co.kr/position/40577, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/40576, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/40451, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/39434, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39433, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39430, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39429, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/40433, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/39332, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/40133, D-day: 23\n",
      "링크: https://jumpit.saramin.co.kr/position/39375, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39376, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39377, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39391, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/39190, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39061, D-day: 11\n",
      "링크: https://jumpit.saramin.co.kr/position/38170, D-day: 4\n",
      "링크: https://jumpit.saramin.co.kr/position/40313, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/38878, D-day: 10\n",
      "링크: https://jumpit.saramin.co.kr/position/40096, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/40107, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/39952, D-day: 19\n",
      "링크: https://jumpit.saramin.co.kr/position/39613, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39612, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39182, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39181, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/40409, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/40408, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/40357, D-day: 29\n",
      "링크: https://jumpit.saramin.co.kr/position/39718, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39284, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39228, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39227, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/40259, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/39814, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/40420, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/40419, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/39265, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/40236, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/38596, D-day: 9\n",
      "링크: https://jumpit.saramin.co.kr/position/39623, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/39393, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/39264, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/38073, D-day: 2\n",
      "링크: https://jumpit.saramin.co.kr/position/40039, D-day: 21\n",
      "링크: https://jumpit.saramin.co.kr/position/39420, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/39327, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39628, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/39357, D-day: 41\n",
      "링크: https://jumpit.saramin.co.kr/position/39348, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39349, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39154, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39155, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/40104, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/40002, D-day: 24\n",
      "링크: https://jumpit.saramin.co.kr/position/39597, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/37949, D-day: 1\n",
      "링크: https://jumpit.saramin.co.kr/position/40425, D-day: 11\n",
      "링크: https://jumpit.saramin.co.kr/position/40372, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/39656, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39520, D-day: 13\n",
      "링크: https://jumpit.saramin.co.kr/position/39347, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39238, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39239, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/3877, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/38280, D-day: 8\n",
      "링크: https://jumpit.saramin.co.kr/position/40232, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40111, D-day: 27\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/37903, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/38918, D-day: 7\n",
      "링크: https://jumpit.saramin.co.kr/position/39066, D-day: 10\n",
      "링크: https://jumpit.saramin.co.kr/position/39065, D-day: 10\n",
      "링크: https://jumpit.saramin.co.kr/position/38293, D-day: 3\n",
      "링크: https://jumpit.saramin.co.kr/position/39705, D-day: 18\n",
      "링크: https://jumpit.saramin.co.kr/position/38690, D-day: 8\n",
      "링크: https://jumpit.saramin.co.kr/position/38657, D-day: 9\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/37937, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/40475, D-day: 30\n",
      "링크: https://jumpit.saramin.co.kr/position/39571, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/39472, D-day: 13\n",
      "링크: https://jumpit.saramin.co.kr/position/37837, D-day: 1\n",
      "링크: https://jumpit.saramin.co.kr/position/40569, D-day: 28\n",
      "링크: https://jumpit.saramin.co.kr/position/40562, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/40477, D-day: 30\n",
      "링크: https://jumpit.saramin.co.kr/position/40455, D-day: 26\n",
      "링크: https://jumpit.saramin.co.kr/position/40248, D-day: 20\n",
      "링크: https://jumpit.saramin.co.kr/position/40045, D-day: 20\n",
      "링크: https://jumpit.saramin.co.kr/position/39016, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39017, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/40218, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/39481, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39484, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39459, D-day: 13\n",
      "링크: https://jumpit.saramin.co.kr/position/39436, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39435, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39289, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39290, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39291, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39286, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39287, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/38920, D-day: 10\n",
      "링크: https://jumpit.saramin.co.kr/position/38921, D-day: 10\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/37934, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/40234, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/39026, D-day: 11\n",
      "링크: https://jumpit.saramin.co.kr/position/39011, D-day: 11\n",
      "링크: https://jumpit.saramin.co.kr/position/38674, D-day: 8\n",
      "링크: https://jumpit.saramin.co.kr/position/40413, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/40411, D-day: 27\n",
      "링크: https://jumpit.saramin.co.kr/position/40131, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40203, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40201, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40200, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40199, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40198, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40089, D-day: 23\n",
      "링크: https://jumpit.saramin.co.kr/position/40090, D-day: 23\n",
      "링크: https://jumpit.saramin.co.kr/position/39480, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39241, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/39243, D-day: 14\n",
      "링크: https://jumpit.saramin.co.kr/position/38235, D-day: 2\n",
      "링크: https://jumpit.saramin.co.kr/position/39950, D-day: 19\n",
      "링크: https://jumpit.saramin.co.kr/position/39948, D-day: 19\n",
      "링크: https://jumpit.saramin.co.kr/position/38996, D-day: 10\n",
      "링크: https://jumpit.saramin.co.kr/position/38271, D-day: 2\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/38018, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/40353, D-day: 28\n",
      "링크: https://jumpit.saramin.co.kr/position/38834, D-day: 9\n",
      "D-day 추출 실패: invalid literal for int() with base 10: 'day'\n",
      "링크: https://jumpit.saramin.co.kr/position/37933, D-day: None\n",
      "링크: https://jumpit.saramin.co.kr/position/39103, D-day: 7\n",
      "링크: https://jumpit.saramin.co.kr/position/39900, D-day: 19\n",
      "링크: https://jumpit.saramin.co.kr/position/39626, D-day: 16\n",
      "링크: https://jumpit.saramin.co.kr/position/38291, D-day: 3\n",
      "링크: https://jumpit.saramin.co.kr/position/40206, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/40204, D-day: 25\n",
      "링크: https://jumpit.saramin.co.kr/position/39672, D-day: 17\n",
      "링크: https://jumpit.saramin.co.kr/position/39467, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39387, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39386, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39385, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39383, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39382, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39381, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39380, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39379, D-day: 12\n",
      "링크: https://jumpit.saramin.co.kr/position/39167, D-day: 11\n",
      "링크: https://jumpit.saramin.co.kr/position/39166, D-day: 11\n",
      "링크: https://jumpit.saramin.co.kr/position/40047, D-day: 20\n",
      "링크: https://jumpit.saramin.co.kr/position/39940, D-day: 19\n",
      "링크: https://jumpit.saramin.co.kr/position/39469, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39468, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39466, D-day: 15\n",
      "링크: https://jumpit.saramin.co.kr/position/39465, D-day: 15\n",
      "총 218개의 링크와 D-day 정보가 저장되었습니다. 파일: ./jumpit/20241222.log\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# 검색 키워드 설정\n",
    "search_keyword = \"데이터 엔지니어\"\n",
    "\n",
    "# Selenium 웹 드라이버 설정\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Jumpit 페이지 열기\n",
    "driver.get(\"https://jumpit.saramin.co.kr/\")\n",
    "\n",
    "# 페이지 로딩 대기\n",
    "time.sleep(5)\n",
    "\n",
    "# 팝업 닫기\n",
    "try:\n",
    "    close_popup = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.sc-c82c57bb-4.cLpfjF\"))\n",
    "    )\n",
    "    close_popup.click()\n",
    "    print(\"팝업 닫음.\")\n",
    "except Exception as e:\n",
    "    print(f\"팝업 닫기 실패: {e}\")\n",
    "\n",
    "# 검색창 찾기 및 검색\n",
    "try:\n",
    "    search_input = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='검색어를 입력해주세요']\")\n",
    "    search_input.send_keys(search_keyword)  # 검색어 입력\n",
    "    search_input.send_keys(Keys.RETURN)  # Enter 키 입력\n",
    "    print(f\"'{search_keyword}' 검색어 입력 완료.\")\n",
    "except Exception as e:\n",
    "    print(f\"검색창을 찾을 수 없습니다: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# 검색 결과 로딩 대기\n",
    "time.sleep(5)\n",
    "\n",
    "# 링크와 D-day 값을 저장할 딕셔너리\n",
    "link_data = {}\n",
    "\n",
    "# 정규 표현식 패턴 (position/ 뒤에 숫자가 있는 URL을 찾는 패턴)\n",
    "pattern = re.compile(r'position/\\d+$')\n",
    "\n",
    "# 스크롤 내리기 및 링크 추출 반복\n",
    "previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    for link in links:\n",
    "        href = link.get_attribute(\"href\")\n",
    "        if href and pattern.search(href) and href not in link_data:\n",
    "            # D-day 정보 추출\n",
    "            try:\n",
    "                d_day_elements = link.find_elements(By.XPATH, \".//div[contains(@class, 'sc-d609d44f-3')]/span\")\n",
    "                if d_day_elements:\n",
    "                    d_day_text = d_day_elements[0].text.strip()\n",
    "                    if \"상시\" in d_day_text:\n",
    "                        d_day = None  # 상시는 null로 저장\n",
    "                    elif d_day_text.startswith(\"D-\"):\n",
    "                        d_day = int(d_day_text[2:])  # D-숫자에서 숫자만 추출\n",
    "                    else:\n",
    "                        d_day = None\n",
    "                else:\n",
    "                    d_day = None\n",
    "            except Exception as e:\n",
    "                print(f\"D-day 추출 실패: {e}\")\n",
    "                d_day = None\n",
    "\n",
    "            link_data[href] = d_day\n",
    "            print(f\"링크: {href}, D-day: {d_day}\")\n",
    "\n",
    "    # 스크롤 내리기\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 새로운 높이 확인\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == previous_height:\n",
    "        break\n",
    "    previous_height = new_height\n",
    "\n",
    "# 디렉터리 생성 (로컬 저장용 디렉터리)\n",
    "output_folder = \"./jumpit\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 오늘 날짜 기반 파일 이름 생성\n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "output_file_path = os.path.join(output_folder, f\"{today}.log\")\n",
    "log_file_name = os.path.join(output_folder, f\"{today}.log\")\n",
    "\n",
    "yesterday = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "yesterday_log_file = os.path.join(output_folder, f\"{yesterday}.log\")\n",
    "\n",
    "# 어제 로그 파일이 있으면 읽기\n",
    "previous_urls = {}\n",
    "if os.path.exists(yesterday_log_file):\n",
    "    with open(yesterday_log_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file.readlines()[1:]:  # 첫 번째 줄은 header\n",
    "            columns = line.strip().split(',')\n",
    "            url = columns[0]\n",
    "            notice_status = columns[1]\n",
    "            work_status = columns[2]\n",
    "            done_time = columns[3]\n",
    "            previous_urls[url] = {\n",
    "                'notice_status': notice_status,\n",
    "                'work_status': work_status,\n",
    "                'done_time': done_time\n",
    "            }\n",
    "\n",
    "# 오늘 로그 파일에 기록할 내용 생성\n",
    "log_data = []\n",
    "\n",
    "# 오늘 크롤링한 URL과 어제 로그 파일을 비교하여 상태 설정\n",
    "for url, d_day in link_data.items():\n",
    "    if os.path.exists(yesterday_log_file):\n",
    "        if url in previous_urls:\n",
    "            if previous_urls[url]['work_status'] != \"done\":\n",
    "                notice_status = previous_urls[url]['notice_status']\n",
    "                work_status = previous_urls[url]['work_status']\n",
    "                done_time = previous_urls[url]['done_time']\n",
    "            else:\n",
    "                notice_status = \"exist\"\n",
    "                work_status = \"done\"\n",
    "                done_time = previous_urls[url]['done_time']\n",
    "        else:\n",
    "            notice_status = \"deleted\"\n",
    "            work_status = \"done\"\n",
    "            done_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        notice_status = \"update\"\n",
    "        work_status = \"null\"\n",
    "        done_time = \"null\"\n",
    "\n",
    "    # D-day 값을 로그 데이터의 마지막 열로 추가\n",
    "    log_data.append(f\"{url},{notice_status},{work_status},{done_time},{d_day if d_day is not None else 'null'}\")\n",
    "\n",
    "# 오늘 로그 파일 생성 (기존 로그 파일 덮어쓰기)\n",
    "with open(log_file_name, 'w', encoding='utf-8') as file:\n",
    "    # 헤더 작성\n",
    "    file.write(\"url,notice_status,work_status,done_time,d_day\\n\")\n",
    "    for line in log_data:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "print(f\"총 {len(log_data)}개의 링크와 D-day 정보가 저장되었습니다. 파일: {output_file_path}\")\n",
    "\n",
    "# Selenium 종료\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# 점핏 완료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout on page 8\n",
      "✅ 링크 파일 20241222.txt이 로컬에 성공적으로 저장되었습니다\n",
      "Timeout on page 8\n",
      "✅ 링크 파일 20241222.txt이 로컬에 성공적으로 저장되었습니다\n",
      "Timeout on page 5\n",
      "✅ 링크 파일 20241222.txt이 로컬에 성공적으로 저장되었습니다\n",
      "Timeout on page 6\n",
      "✅ 링크 파일 20241222.txt이 로컬에 성공적으로 저장되었습니다\n",
      "Timeout on page 4\n",
      "✅ 링크 파일 20241222.txt이 로컬에 성공적으로 저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from zoneinfo import ZoneInfo\n",
    "import re\n",
    "\n",
    "# 변수 설정 : 검색 키워드\n",
    "job_titles = {\n",
    "    \"DE\": \"데이터 엔지니어\",\n",
    "    \"DA\": \"데이터 분석가\",\n",
    "    \"FE\": \"프론트엔드 엔지니어\",\n",
    "    \"BE\": \"백엔드 엔지니어\",\n",
    "    \"MLE\": \"머신러닝 엔지니어\"\n",
    "}\n",
    "\n",
    "# 변수 설정 : 로컬 저장 경로\n",
    "LOCAL_PATH = './rocketpunch/'\n",
    "\n",
    "kst = ZoneInfo(\"Asia/Seoul\")\n",
    "\n",
    "# Chrome driver 옵션 설정\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Headless 모드\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 비활성화\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # 리소스 제약 없는 환경에서 실행\n",
    "\n",
    "def get_total_pages(base_url):\n",
    "    \"\"\" Rocketpunch의 유효한 페이지 수를 반환하는 함수.\"\"\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    try:\n",
    "        page = 1\n",
    "        while True:\n",
    "            page_url = f\"{base_url}&page={page}\"\n",
    "            driver.get(page_url)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#search-results > div.ui.job.items.segment.company-list > div.company.item'))\n",
    "            )\n",
    "            job_elements = driver.find_elements(By.CSS_SELECTOR, '#search-results > div.ui.job.items.segment.company-list > div.company.item')\n",
    "            if not job_elements:\n",
    "                print(f\"Page {page} has no job postings. Total pages: {page - 1}\")\n",
    "                return page - 1\n",
    "            page += 1\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout on page {page}\")\n",
    "        return page - 1\n",
    "    except WebDriverException as e:\n",
    "        print(f\"WebDriverException: {e}\")\n",
    "        return page - 1\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def get_links_from_page(page_url):\n",
    "    \"\"\" \n",
    "    한 페이지에서 채용공고 링크를 크롤링하는 함수\n",
    "    :param page_url: 특정 페이지 URL\n",
    "    :return: 페이지 내 공고 링크 리스트\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    try:\n",
    "        driver.get(page_url)\n",
    "\n",
    "        # 모든 공고가 로드될 때까지 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#search-results > div.ui.job.items.segment.company-list > div.company.item'))\n",
    "        )\n",
    "\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        pattern = re.compile(r\"^https://www\\.rocketpunch\\.com/jobs/\\d+/[\\w\\-\\%]+\")\n",
    "        page_links = [link.get_attribute(\"href\") for link in links if link.get_attribute(\"href\") and pattern.search(link.get_attribute(\"href\"))]\n",
    "        return page_links\n",
    "    \n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout while loading page: {page_url}\")\n",
    "        return []\n",
    "    except WebDriverException as e:\n",
    "        print(f\"WebDriver error while accessing page: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def get_all_links(keyword):\n",
    "    \"\"\" 모든 페이지에서 채용공고 링크를 수집하는 함수, 나중에 직무(DA, DE, FE, BE...) 에 따라 공고 링크 수집\"\"\"\n",
    "    all_posts_links = []  # 데이터 엔지니어 공고 링크 리스트\n",
    "\n",
    "    keyword = keyword.replace(\" \", \"+\")\n",
    "    base_url = f\"https://www.rocketpunch.com/jobs?keywords={keyword}\"  # \"데이터\" || \"엔지니어\"\n",
    "    \n",
    "    try:\n",
    "        total_pages = get_total_pages(base_url)\n",
    "        if total_pages == 0:\n",
    "            print(f\"키워드 {keyword}에 해당하는 채용공고가 없습니다!\")\n",
    "            return []\n",
    "\n",
    "        for page in range(1, total_pages + 1):\n",
    "            page_url = f\"{base_url}&page={page}\"\n",
    "            page_links = get_links_from_page(page_url)\n",
    "            all_posts_links.extend(page_links)\n",
    "\n",
    "        return all_posts_links\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_link_to_local(local_path, today_date, today_links):\n",
    "    \"\"\" 로컬에 파일을 저장하는 함수\n",
    "    :param local_path: 로컬 디렉터리 경로\n",
    "    :param today_date: 오늘 날짜 (파일 이름)\n",
    "    :param today_links: 저장할 링크 리스트\n",
    "    :return: 파일 저장 성공 여부\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(local_path)  # 경로가 없으면 생성\n",
    "        \n",
    "        file_name = f\"{today_date}.txt\"\n",
    "        file_path = os.path.join(local_path, file_name)\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(\"\\n\".join(today_links))\n",
    "        \n",
    "        print(f\"✅ 링크 파일 {file_name}이 로컬에 성공적으로 저장되었습니다\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⛔ [ERROR] 로컬로 파일을 저장하는데 에러 발생 {e}\")\n",
    "        return False\n",
    "\n",
    "# 코드가 바로 실행되도록 변경된 부분\n",
    "for job_abb, job_title in job_titles.items():\n",
    "    # 로컬 링크 저장 경로 설정\n",
    "    local_path = os.path.join(LOCAL_PATH, job_abb)\n",
    "    today_links = get_all_links(job_title)\n",
    "\n",
    "    today_date = datetime.now(tz=kst).strftime('%Y%m%d')\n",
    "    # 오늘 수집한 링크를 로컬에 저장\n",
    "    save_link_to_local(local_path, today_date, today_links)\n",
    "# 로켓펀치 완료 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m options \u001b[38;5;241m=\u001b[39m Options()\n\u001b[1;32m     74\u001b[0m options\u001b[38;5;241m.\u001b[39mheadless \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 드라이버를 헤드리스 모드로 실행할 수 있음 (True로 설정하여 브라우저를 표시하지 않게 할 수 있음)\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m log_process(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChrome WebDriver started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 오늘 날짜로 로그 파일 이름 설정\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/chromium/webdriver.py:66\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[1;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:238\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticator_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_client()\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_websocket_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:325\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new session with the desired capabilities.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m:Args:\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m - capabilities - a capabilities dict to start the session with.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m caps \u001b[38;5;241m=\u001b[39m _create_caps(capabilities)\n\u001b[0;32m--> 325\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_SESSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:378\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    376\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 378\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:391\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    389\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    390\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:415\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    412\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 415\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import unquote\n",
    "\n",
    "if not os.path.exists('./saramin'):\n",
    "    os.makedirs('./saramin')  # 디렉토리 생성\n",
    "\n",
    "def log_error(error_message):\n",
    "    \"\"\"오류를 날짜별로 makelog_err_YYYYMMDD.log 파일에 기록\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')  # 날짜 포맷\n",
    "    log_filename = f'./saramin/makelog_err_{today}.log'  # 파일명에 날짜 추가\n",
    "    with open(log_filename, 'a', encoding='utf-8') as err_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        err_file.write(f\"{timestamp},{error_message}\\n\")\n",
    "\n",
    "def log_process(process_message):\n",
    "    \"\"\"프로세스 로그를 날짜별로 makelog_process_YYYYMMDD.log 파일에 기록\"\"\"\n",
    "    today = datetime.today().strftime('%Y%m%d')  # 날짜 포맷\n",
    "    log_filename = f'./saramin/makelog_process_{today}.log'  # 파일명에 날짜 추가\n",
    "    with open(log_filename, 'a', encoding='utf-8') as process_file:\n",
    "        timestamp = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        process_file.write(f\"{timestamp},{process_message}\\n\")\n",
    "\n",
    "def convert_html_entities_to_url(html_link):\n",
    "    \"\"\"HTML 엔티티를 URL로 변환하고, 상대경로를 절대경로로 변환\"\"\"\n",
    "    # HTML 엔티티 &amp;를 &로 변환\n",
    "    html_link = html_link.replace('&amp;', '&')\n",
    "\n",
    "    # 상대 경로에 도메인 추가\n",
    "    base_url = \"https://www.saramin.co.kr\"\n",
    "    if html_link.startswith('/'):\n",
    "        html_link = base_url + html_link\n",
    "\n",
    "    # URL 인코딩된 값 복원 (예: %2F -> /)\n",
    "    html_link = unquote(html_link)\n",
    "\n",
    "    return html_link\n",
    "\n",
    "def has_uuid(url):\n",
    "    \"\"\"URL에 uuid가 포함되어 있는지 확인\"\"\"\n",
    "    return bool(re.search(r'[0-9a-fA-F-]{36}', url))  # UUID 형식을 정규표현식으로 검사\n",
    "\n",
    "# 글로벌 변수 선언\n",
    "job_url_list = {\n",
    "    \"DE\": [\n",
    "        \"https://www.saramin.co.kr/zf_user/search/recruit?search_area=main&search_done=y&search_optional_item=n&searchType=recently&searchword=데이터%20엔지니어&recruitPage={}&recruitSort=relation&recruitPageCount=40&inner_com_type=&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7%2C9%2C10&show_applied=&quick_apply=&except_read=&ai_head_hunting=&mainSearch=n\",\n",
    "    ],\n",
    "    \"FE\": [\n",
    "        \"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7%2C9%2C10&keydownAccess=&searchword=프론트엔드&panel_type=&search_optional_item=y&search_done=y&panel_count=y&preview=y&recruitPage={}&recruitSort=relation&recruitPageCount=40&inner_com_type=&show_applied=&quick_apply=&except_read=&ai_head_hunting=\",\n",
    "    ],\n",
    "    \"BE\": [\n",
    "        \"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7%2C9%2C10&keydownAccess=&searchword=백엔드&panel_type=&search_optional_item=y&search_done=y&panel_count=y&preview=y&recruitPage={}&recruitSort=relation&recruitPageCount=40&inner_com_type=&show_applied=&quick_apply=&except_read=&ai_head_hunting=\",\n",
    "    ],\n",
    "    \"MLE\": [\n",
    "        \"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7%2C9%2C10&keydownAccess=&searchword=머신러닝%20엔지니어&panel_type=&search_optional_item=y&search_done=y&panel_count=y&preview=y&recruitPage={}&recruitSort=relation&recruitPageCount=40&inner_com_type=&show_applied=&quick_apply=&except_read=&ai_head_hunting=\",\n",
    "    ],\n",
    "    \"DA\": [\n",
    "        \"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7%2C9%2C10&keydownAccess=&searchword=데이터%20분석가&panel_type=&search_optional_item=y&search_done=y&panel_count=y&preview=y&recruitPage={}&recruitSort=relation&recruitPageCount=40&inner_com_type=&show_applied=&quick_apply=&except_read=&ai_head_hunting=\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 셀레니움 웹 드라이버 설정\n",
    "    options = Options()\n",
    "    options.headless = False  # 드라이버를 헤드리스 모드로 실행할 수 있음 (True로 설정하여 브라우저를 표시하지 않게 할 수 있음)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    log_process(\"Chrome WebDriver started.\")\n",
    "\n",
    "    # 오늘 날짜로 로그 파일 이름 설정\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "    today_log_file_name = f\"./saramin/{today}.log\"\n",
    "\n",
    "    # 로그 파일을 찾을 디렉토리 설정\n",
    "    log_directory = './saramin'  # 원하는 디렉토리로 변경\n",
    "    log_files = [f for f in os.listdir(log_directory) if re.match(r'^\\d{8}\\.log$', f)]\n",
    "\n",
    "    # 가장 최근에 생성된 로그 파일 찾기\n",
    "    if log_files:\n",
    "        log_files.sort(key=lambda x: os.path.getmtime(os.path.join(log_directory, x)), reverse=True)\n",
    "        recent_log_file_name = log_files[0]  # 가장 최근의 로그 파일을 선택\n",
    "        log_process(f\"Found the most recent log file: {recent_log_file_name}\")\n",
    "    else:\n",
    "        log_process(\"No log files found in the directory. All URLs will be marked as 'update'.\")\n",
    "        recent_log_file_name = None  # recent_log_file_name을 None으로 설정\n",
    "\n",
    "    # 이전 로그 파일이 존재하는지 확인하고 읽기\n",
    "    previous_urls = {}  # 이전 로그에 있는 URL 및 해당 job\n",
    "    if recent_log_file_name and os.path.exists(os.path.join(log_directory, recent_log_file_name)):\n",
    "        with open(os.path.join(log_directory, recent_log_file_name), 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            if lines:  # 파일에 내용이 있을 때만 처리\n",
    "                for line in lines[1:]:  # 첫 번째 줄은 header\n",
    "                    columns = line.strip().split(',')\n",
    "                    if len(columns) >= 5:  # 만약 각 열이 다 있다면\n",
    "                        url = columns[1]\n",
    "                        job = columns[0]  # 해당 URL의 job\n",
    "                        notice_status = columns[2]\n",
    "                        work_status = columns[3]\n",
    "                        done_time = columns[4]\n",
    "                        if notice_status != \"deleted\":  # \"deleted\" 상태인 URL은 제외\n",
    "                            previous_urls[url] = {'job': job, 'notice_status': notice_status, 'work_status': work_status, 'done_time': done_time}  # URL에 해당하는 정보 저장\n",
    "\n",
    "    # 오늘 크롤링한 URL을 수집\n",
    "    all_links = []  # 오늘 수집한 모든 링크\n",
    "    job_for_links = {}  # 각 링크에 해당하는 job을 기록하기 위한 dictionary\n",
    "\n",
    "    # 각 job (키값)에 대한 URL 처리\n",
    "    for job_key, urls in job_url_list.items():\n",
    "        for url in urls:\n",
    "            log_process(f\"Processing job: {job_key}, URL: {url}\")\n",
    "            for page_number in range(1, 41):  # 1부터 40까지 페이지 크롤링\n",
    "                page_url = url.format(page_number)\n",
    "                driver.get(page_url)\n",
    "\n",
    "                # 페이지 로딩 대기\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "\n",
    "                # 링크 추출\n",
    "                links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href:\n",
    "                        converted_url = convert_html_entities_to_url(href)\n",
    "\n",
    "                        # UUID가 포함된 링크만 추가\n",
    "                        if has_uuid(converted_url) and converted_url not in all_links:\n",
    "                            all_links.append(converted_url)\n",
    "                            job_for_links[converted_url] = job_key  # 링크와 job 정보를 매핑\n",
    "\n",
    "    # 크롤링 완료 후, 링크 정보를 로그로 기록\n",
    "    for link in all_links:\n",
    "        job = job_for_links[link]\n",
    "        log_process(f\"{job}, {link}\")\n",
    "\n",
    "    # 오늘 크롤링한 URL과 이전 로그 파일을 비교하여 상태 설정\n",
    "    log_data_deleted = []  # deleted 상태를 따로 저장\n",
    "    log_data_other = []    # 나머지 (exist, update) 상태를 따로 저장\n",
    "\n",
    "    # 1. 오늘 크롤링한 URL과 이전 로그 파일의 비교\n",
    "    for url in all_links:\n",
    "        if url in previous_urls:\n",
    "            # 이전 로그 파일과 오늘 모두 존재하는 URL이면 \"exist\"로 처리\n",
    "            if previous_urls[url]['notice_status'] == \"deleted\":\n",
    "                continue\n",
    "            notice_status = \"exist\"\n",
    "            work_status = previous_urls[url]['work_status']\n",
    "            done_time = previous_urls[url]['done_time']\n",
    "            job = previous_urls[url]['job']\n",
    "            log_data_other.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "        else:\n",
    "            # 오늘만 존재하는 URL은 \"update\"로 설정\n",
    "            notice_status = \"update\"\n",
    "            work_status = \"null\"\n",
    "            done_time = \"null\"\n",
    "            job = job_for_links[url]\n",
    "            log_data_other.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "\n",
    "    # 2. 이전 로그 파일에 있지만 오늘 로그 파일에 없는 URL 처리\n",
    "    for url in previous_urls:\n",
    "        if url not in all_links:\n",
    "            notice_status = \"deleted\"\n",
    "            work_status = \"done\"\n",
    "            done_time = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            job = previous_urls[url]['job']\n",
    "            log_data_deleted.append(f\"{job},{url},{notice_status},{work_status},{done_time}\")\n",
    "\n",
    "    # 로그 파일 저장\n",
    "    with open(today_log_file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"job,url,notice_status,work_status,done_time\\n\")\n",
    "        for line in log_data_deleted:\n",
    "            file.write(line + \"\\n\")\n",
    "        for line in log_data_other:\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "    log_process(f\"Log file saved: {today_log_file_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 오류 발생 시 오류 메시지 기록\n",
    "    error_message = str(e)\n",
    "    log_error(error_message)\n",
    "    log_process(f\"Error occurred: {error_message}\")\n",
    "    if 'driver' in locals():\n",
    "        driver.quit()\n",
    "        log_process(\"Chrome WebDriver closed after error.\")\n",
    "\n",
    "finally:\n",
    "    # 드라이버 종료 (예외 없이 처리되었을 경우에도)\n",
    "    if 'driver' in locals():\n",
    "        driver.quit()\n",
    "        log_process(\"Chrome WebDriver closed.\")   \n",
    "\n",
    "# saramin done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
